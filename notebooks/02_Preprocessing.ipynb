{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîß Preprocesamiento de Texto\n",
        "## Pipeline completo de limpieza y normalizaci√≥n\n",
        "\n",
        "### Objetivos:\n",
        "1. Implementar pipeline de preprocesamiento completo\n",
        "2. Limpiar y normalizar el texto\n",
        "3. Aplicar tokenizaci√≥n, eliminaci√≥n de stopwords y lematizaci√≥n\n",
        "4. Guardar datos preprocesados\n",
        "5. Comparar texto original vs preprocesado\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importar librer√≠as\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# A√±adir src al path para importar m√≥dulos\n",
        "sys.path.append(str(Path('../src').resolve()))\n",
        "\n",
        "from data.preprocessing import TextPreprocessor\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cargar datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar dataset\n",
        "data_path = Path('../data/raw/youtoxic_english_1000.csv')\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "print(f\"‚úÖ Dataset cargado: {len(df)} filas, {len(df.columns)} columnas\")\n",
        "print(f\"\\nüìã Primeras filas:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Identificar columnas de texto y etiquetas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identificar columna de texto\n",
        "text_col = 'Text'  # Seg√∫n el EDA, esta es la columna de texto\n",
        "label_col = 'IsToxic'  # Columna principal de etiquetas\n",
        "\n",
        "# Verificar que existen\n",
        "assert text_col in df.columns, f\"Columna '{text_col}' no encontrada\"\n",
        "assert label_col in df.columns, f\"Columna '{label_col}' no encontrada\"\n",
        "\n",
        "print(f\"‚úÖ Columna de texto: '{text_col}'\")\n",
        "print(f\"‚úÖ Columna de etiquetas: '{label_col}'\")\n",
        "\n",
        "# Verificar valores nulos\n",
        "print(f\"\\nüîç Valores nulos en '{text_col}': {df[text_col].isnull().sum()}\")\n",
        "print(f\"üîç Valores nulos en '{label_col}': {df[label_col].isnull().sum()}\")\n",
        "\n",
        "# Eliminar filas con texto nulo\n",
        "df = df.dropna(subset=[text_col])\n",
        "print(f\"\\n‚úÖ Dataset despu√©s de eliminar nulos: {len(df)} filas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Inicializar preprocesador\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicializar preprocesador (usa spaCy si est√° disponible)\n",
        "preprocessor = TextPreprocessor(use_spacy=True)\n",
        "\n",
        "print(\"‚úÖ Preprocesador inicializado\")\n",
        "print(f\"   Usando: {'spaCy' if preprocessor.use_spacy else 'NLTK'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Ejemplos de preprocesamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostrar ejemplos de preprocesamiento\n",
        "print(\"=\"*80)\n",
        "print(\"EJEMPLOS DE PREPROCESAMIENTO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "sample_texts = df[text_col].sample(5, random_state=42).tolist()\n",
        "\n",
        "for i, text in enumerate(sample_texts, 1):\n",
        "    processed = preprocessor.preprocess_text(text, remove_stopwords=True)\n",
        "    print(f\"\\nüìù Ejemplo {i}:\")\n",
        "    print(f\"   Original:  {text[:150]}...\" if len(text) > 150 else f\"   Original:  {text}\")\n",
        "    print(f\"   Procesado: {processed[:150]}...\" if len(processed) > 150 else f\"   Procesado: {processed}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Aplicar preprocesamiento a todo el dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplicar preprocesamiento\n",
        "print(\"=\"*80)\n",
        "print(\"PREPROCESANDO DATASET COMPLETO\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚è≥ Esto puede tardar unos minutos...\\n\")\n",
        "\n",
        "df_processed = preprocessor.preprocess_dataframe(\n",
        "    df, \n",
        "    text_column=text_col,\n",
        "    output_column='Text_processed',\n",
        "    remove_stopwords=True,\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Preprocesamiento completado\")\n",
        "print(f\"   Filas procesadas: {len(df_processed)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. An√°lisis del preprocesamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular estad√≠sticas\n",
        "df_processed['text_length_original'] = df_processed[text_col].str.len()\n",
        "df_processed['text_length_processed'] = df_processed['Text_processed'].str.len()\n",
        "df_processed['word_count_original'] = df_processed[text_col].str.split().str.len()\n",
        "df_processed['word_count_processed'] = df_processed['Text_processed'].str.split().str.len()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ESTAD√çSTICAS DE PREPROCESAMIENTO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "stats = pd.DataFrame({\n",
        "    'Original': [\n",
        "        df_processed['text_length_original'].mean(),\n",
        "        df_processed['word_count_original'].mean(),\n",
        "        df_processed['text_length_original'].min(),\n",
        "        df_processed['text_length_original'].max()\n",
        "    ],\n",
        "    'Procesado': [\n",
        "        df_processed['text_length_processed'].mean(),\n",
        "        df_processed['word_count_processed'].mean(),\n",
        "        df_processed['text_length_processed'].min(),\n",
        "        df_processed['text_length_processed'].max()\n",
        "    ]\n",
        "}, index=['Longitud promedio (caracteres)', 'Palabras promedio', 'Longitud m√≠nima', 'Longitud m√°xima'])\n",
        "\n",
        "print(\"\\nüìä Comparaci√≥n:\")\n",
        "print(stats)\n",
        "\n",
        "print(f\"\\nüìâ Reducci√≥n promedio:\")\n",
        "print(f\"   Caracteres: {((df_processed['text_length_original'].mean() - df_processed['text_length_processed'].mean()) / df_processed['text_length_original'].mean() * 100):.1f}%\")\n",
        "print(f\"   Palabras: {((df_processed['word_count_original'].mean() - df_processed['word_count_processed'].mean()) / df_processed['word_count_original'].mean() * 100):.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualizaci√≥n de resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Histograma de longitud original vs procesada\n",
        "axes[0, 0].hist(df_processed['text_length_original'], bins=50, alpha=0.7, label='Original', color='#3498db')\n",
        "axes[0, 0].hist(df_processed['text_length_processed'], bins=50, alpha=0.7, label='Procesado', color='#e74c3c')\n",
        "axes[0, 0].set_xlabel('Longitud (caracteres)')\n",
        "axes[0, 0].set_ylabel('Frecuencia')\n",
        "axes[0, 0].set_title('Distribuci√≥n de Longitud de Texto', fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# Histograma de n√∫mero de palabras\n",
        "axes[0, 1].hist(df_processed['word_count_original'], bins=50, alpha=0.7, label='Original', color='#3498db')\n",
        "axes[0, 1].hist(df_processed['word_count_processed'], bins=50, alpha=0.7, label='Procesado', color='#e74c3c')\n",
        "axes[0, 1].set_xlabel('N√∫mero de palabras')\n",
        "axes[0, 1].set_ylabel('Frecuencia')\n",
        "axes[0, 1].set_title('Distribuci√≥n de N√∫mero de Palabras', fontweight='bold')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# Boxplot comparativo\n",
        "df_box = pd.melt(df_processed, \n",
        "                 value_vars=['text_length_original', 'text_length_processed'],\n",
        "                 var_name='Tipo', value_name='Longitud')\n",
        "df_box['Tipo'] = df_box['Tipo'].replace({'text_length_original': 'Original', 'text_length_processed': 'Procesado'})\n",
        "sns.boxplot(data=df_box, x='Tipo', y='Longitud', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Comparaci√≥n de Longitud', fontweight='bold')\n",
        "axes[1, 0].grid(alpha=0.3, axis='y')\n",
        "\n",
        "# Scatter plot: original vs procesado\n",
        "axes[1, 1].scatter(df_processed['text_length_original'], df_processed['text_length_processed'], \n",
        "                   alpha=0.5, s=10, color='#2ecc71')\n",
        "axes[1, 1].plot([0, df_processed['text_length_original'].max()], \n",
        "                [0, df_processed['text_length_original'].max()], \n",
        "                'r--', linewidth=2, label='L√≠nea de igualdad')\n",
        "axes[1, 1].set_xlabel('Longitud Original')\n",
        "axes[1, 1].set_ylabel('Longitud Procesada')\n",
        "axes[1, 1].set_title('Relaci√≥n Original vs Procesado', fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar dataset preprocesado\n",
        "output_path = Path('../data/processed/youtoxic_english_1000_processed.csv')\n",
        "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Guardar solo columnas necesarias\n",
        "columns_to_save = ['CommentId', 'VideoId', text_col, 'Text_processed', label_col]\n",
        "if 'IsAbusive' in df_processed.columns:\n",
        "    columns_to_save.append('IsAbusive')\n",
        "if 'IsHatespeech' in df_processed.columns:\n",
        "    columns_to_save.append('IsHatespeech')\n",
        "\n",
        "df_processed[columns_to_save].to_csv(output_path, index=False)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATOS GUARDADOS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"‚úÖ Dataset preprocesado guardado en: {output_path}\")\n",
        "print(f\"   Filas: {len(df_processed)}\")\n",
        "print(f\"   Columnas: {len(columns_to_save)}\")\n",
        "print(f\"\\nüìã Columnas guardadas: {columns_to_save}\")\n",
        "\n",
        "# Mostrar muestra\n",
        "print(f\"\\nüìã Muestra de datos guardados:\")\n",
        "df_processed[columns_to_save].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Resumen del preprocesamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"RESUMEN DEL PREPROCESAMIENTO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n‚úÖ Pipeline aplicado:\")\n",
        "print(f\"   1. Limpieza b√°sica (URLs, emails, caracteres especiales)\")\n",
        "print(f\"   2. Normalizaci√≥n (contracciones, repeticiones)\")\n",
        "print(f\"   3. Tokenizaci√≥n ({'spaCy' if preprocessor.use_spacy else 'NLTK'})\")\n",
        "print(f\"   4. Eliminaci√≥n de stopwords\")\n",
        "print(f\"   5. Lematizaci√≥n\")\n",
        "\n",
        "print(f\"\\nüìä Resultados:\")\n",
        "print(f\"   - Textos procesados: {len(df_processed)}\")\n",
        "print(f\"   - Reducci√≥n promedio de caracteres: {((df_processed['text_length_original'].mean() - df_processed['text_length_processed'].mean()) / df_processed['text_length_original'].mean() * 100):.1f}%\")\n",
        "print(f\"   - Reducci√≥n promedio de palabras: {((df_processed['word_count_original'].mean() - df_processed['word_count_processed'].mean()) / df_processed['word_count_original'].mean() * 100):.1f}%\")\n",
        "\n",
        "print(f\"\\nüíæ Archivo guardado:\")\n",
        "print(f\"   {output_path}\")\n",
        "\n",
        "print(\"\\n‚úÖ Preprocesamiento completado exitosamente\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
