{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Modelado Baseline: Modelos Cl√°sicos de ML\n",
        "## Entrenamiento y evaluaci√≥n de modelos de clasificaci√≥n\n",
        "\n",
        "### Objetivos:\n",
        "1. Entrenar modelos cl√°sicos (Naive Bayes, Logistic Regression, SVM, Random Forest)\n",
        "2. Comparar TF-IDF vs Count Vectorizer\n",
        "3. Evaluar m√©tricas (F1, Accuracy, Precision, Recall)\n",
        "4. Analizar overfitting\n",
        "5. Seleccionar mejor modelo baseline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importar librer√≠as\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# A√±adir src al path\n",
        "sys.path.append(str(Path('../src').resolve()))\n",
        "\n",
        "from models.train import train_model, save_model\n",
        "from models.evaluate import evaluate_model, compare_models, print_classification_report\n",
        "from features.vectorization import load_vectorized_data\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cargar datos vectorizados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar datos vectorizados con TF-IDF\n",
        "data_dir = Path('../data/processed')\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = load_vectorized_data(data_dir, prefix='tfidf')\n",
        "\n",
        "# Cargar datos vectorizados con Count Vectorizer\n",
        "X_train_count, X_test_count, _, _ = load_vectorized_data(data_dir, prefix='count')\n",
        "\n",
        "print(f\"\\n‚úÖ Datos cargados:\")\n",
        "print(f\"   TF-IDF - Train: {X_train_tfidf.shape}, Test: {X_test_tfidf.shape}\")\n",
        "print(f\"   Count  - Train: {X_train_count.shape}, Test: {X_test_count.shape}\")\n",
        "print(f\"   Labels - Train: {len(y_train)}, Test: {len(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Entrenar modelos con TF-IDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"ENTRENANDO MODELOS CON TF-IDF\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "models_tfidf = {}\n",
        "results_tfidf = {}\n",
        "\n",
        "# 1. Naive Bayes\n",
        "print(\"\\nüîµ Entrenando Naive Bayes...\")\n",
        "models_tfidf['Naive Bayes'] = train_model('naive_bayes', X_train_tfidf, y_train, alpha=1.0)\n",
        "results_tfidf['Naive Bayes'] = evaluate_model(\n",
        "    models_tfidf['Naive Bayes'], X_train_tfidf, X_test_tfidf, y_train, y_test\n",
        ")\n",
        "\n",
        "# 2. Logistic Regression\n",
        "print(\"\\nüü¢ Entrenando Logistic Regression...\")\n",
        "models_tfidf['Logistic Regression'] = train_model(\n",
        "    'logistic', X_train_tfidf, y_train, C=1.0, penalty='l2', class_weight='balanced'\n",
        ")\n",
        "results_tfidf['Logistic Regression'] = evaluate_model(\n",
        "    models_tfidf['Logistic Regression'], X_train_tfidf, X_test_tfidf, y_train, y_test\n",
        ")\n",
        "\n",
        "# 3. SVM\n",
        "print(\"\\nüü° Entrenando SVM...\")\n",
        "models_tfidf['SVM'] = train_model(\n",
        "    'svm', X_train_tfidf, y_train, C=1.0, kernel='linear', class_weight='balanced'\n",
        ")\n",
        "results_tfidf['SVM'] = evaluate_model(\n",
        "    models_tfidf['SVM'], X_train_tfidf, X_test_tfidf, y_train, y_test\n",
        ")\n",
        "\n",
        "# 4. Random Forest\n",
        "print(\"\\nüî¥ Entrenando Random Forest...\")\n",
        "models_tfidf['Random Forest'] = train_model(\n",
        "    'random_forest', X_train_tfidf, y_train, \n",
        "    n_estimators=100, max_depth=10, class_weight='balanced'\n",
        ")\n",
        "results_tfidf['Random Forest'] = evaluate_model(\n",
        "    models_tfidf['Random Forest'], X_train_tfidf, X_test_tfidf, y_train, y_test\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Entrenar modelos con Count Vectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"ENTRENANDO MODELOS CON COUNT VECTORIZER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "models_count = {}\n",
        "results_count = {}\n",
        "\n",
        "# 1. Naive Bayes\n",
        "print(\"\\nüîµ Entrenando Naive Bayes...\")\n",
        "models_count['Naive Bayes'] = train_model('naive_bayes', X_train_count, y_train, alpha=1.0)\n",
        "results_count['Naive Bayes'] = evaluate_model(\n",
        "    models_count['Naive Bayes'], X_train_count, X_test_count, y_train, y_test\n",
        ")\n",
        "\n",
        "# 2. Logistic Regression\n",
        "print(\"\\nüü¢ Entrenando Logistic Regression...\")\n",
        "models_count['Logistic Regression'] = train_model(\n",
        "    'logistic', X_train_count, y_train, C=1.0, penalty='l2', class_weight='balanced'\n",
        ")\n",
        "results_count['Logistic Regression'] = evaluate_model(\n",
        "    models_count['Logistic Regression'], X_train_count, X_test_count, y_train, y_test\n",
        ")\n",
        "\n",
        "# 3. SVM\n",
        "print(\"\\nüü° Entrenando SVM...\")\n",
        "models_count['SVM'] = train_model(\n",
        "    'svm', X_train_count, y_train, C=1.0, kernel='linear', class_weight='balanced'\n",
        ")\n",
        "results_count['SVM'] = evaluate_model(\n",
        "    models_count['SVM'], X_train_count, X_test_count, y_train, y_test\n",
        ")\n",
        "\n",
        "# 4. Random Forest\n",
        "print(\"\\nüî¥ Entrenando Random Forest...\")\n",
        "models_count['Random Forest'] = train_model(\n",
        "    'random_forest', X_train_count, y_train, \n",
        "    n_estimators=100, max_depth=10, class_weight='balanced'\n",
        ")\n",
        "results_count['Random Forest'] = evaluate_model(\n",
        "    models_count['Random Forest'], X_train_count, X_test_count, y_train, y_test\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"COMPARACI√ìN DE MODELOS - TF-IDF\")\n",
        "print(\"=\"*80)\n",
        "comparison_tfidf = compare_models(results_tfidf)\n",
        "print(\"\\n\" + comparison_tfidf.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARACI√ìN DE MODELOS - COUNT VECTORIZER\")\n",
        "print(\"=\"*80)\n",
        "comparison_count = compare_models(results_count)\n",
        "print(\"\\n\" + comparison_count.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizaci√≥n de resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Comparaci√≥n F1-score\n",
        "comparison_all = pd.concat([\n",
        "    comparison_tfidf.assign(Vectorizer='TF-IDF'),\n",
        "    comparison_count.assign(Vectorizer='Count')\n",
        "])\n",
        "\n",
        "# F1-score por modelo\n",
        "pivot_f1 = comparison_all.pivot(index='Modelo', columns='Vectorizer', values='F1 (test)')\n",
        "pivot_f1.plot(kind='bar', ax=axes[0, 0], color=['#3498db', '#e74c3c'])\n",
        "axes[0, 0].set_title('F1-Score (Test) por Modelo y Vectorizador', fontweight='bold', fontsize=12)\n",
        "axes[0, 0].set_ylabel('F1-Score')\n",
        "axes[0, 0].set_xlabel('Modelo')\n",
        "axes[0, 0].legend(title='Vectorizador')\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Overfitting por modelo\n",
        "pivot_overfitting = comparison_all.pivot(index='Modelo', columns='Vectorizer', values='Overfitting (%)')\n",
        "pivot_overfitting.plot(kind='bar', ax=axes[0, 1], color=['#3498db', '#e74c3c'])\n",
        "axes[0, 1].axhline(y=5, color='r', linestyle='--', label='Objetivo (<5%)')\n",
        "axes[0, 1].set_title('Overfitting por Modelo y Vectorizador', fontweight='bold', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Overfitting (%)')\n",
        "axes[0, 1].set_xlabel('Modelo')\n",
        "axes[0, 1].legend(title='Vectorizador')\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Accuracy por modelo\n",
        "pivot_acc = comparison_all.pivot(index='Modelo', columns='Vectorizer', values='Accuracy (test)')\n",
        "pivot_acc.plot(kind='bar', ax=axes[1, 0], color=['#3498db', '#e74c3c'])\n",
        "axes[1, 0].set_title('Accuracy (Test) por Modelo y Vectorizador', fontweight='bold', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Accuracy')\n",
        "axes[1, 0].set_xlabel('Modelo')\n",
        "axes[1, 0].legend(title='Vectorizador')\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Scatter: F1 vs Overfitting\n",
        "axes[1, 1].scatter(\n",
        "    comparison_all[comparison_all['Vectorizer'] == 'TF-IDF']['Overfitting (%)'],\n",
        "    comparison_all[comparison_all['Vectorizer'] == 'TF-IDF']['F1 (test)'],\n",
        "    label='TF-IDF', s=100, alpha=0.7, color='#3498db'\n",
        ")\n",
        "axes[1, 1].scatter(\n",
        "    comparison_all[comparison_all['Vectorizer'] == 'Count']['Overfitting (%)'],\n",
        "    comparison_all[comparison_all['Vectorizer'] == 'Count']['F1 (test)'],\n",
        "    label='Count', s=100, alpha=0.7, color='#e74c3c', marker='s'\n",
        ")\n",
        "axes[1, 1].axvline(x=5, color='r', linestyle='--', alpha=0.5, label='Objetivo Overfitting')\n",
        "axes[1, 1].set_xlabel('Overfitting (%)')\n",
        "axes[1, 1].set_ylabel('F1-Score (Test)')\n",
        "axes[1, 1].set_title('F1-Score vs Overfitting', fontweight='bold', fontsize=12)\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "# A√±adir etiquetas de modelos\n",
        "for idx, row in comparison_all.iterrows():\n",
        "    if row['Vectorizer'] == 'TF-IDF':\n",
        "        axes[1, 1].annotate(row['Modelo'], \n",
        "                           (row['Overfitting (%)'], row['F1 (test)']),\n",
        "                           fontsize=8, alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combinar todos los resultados\n",
        "all_results = {}\n",
        "for name, results in results_tfidf.items():\n",
        "    all_results[f\"{name} (TF-IDF)\"] = results\n",
        "for name, results in results_count.items():\n",
        "    all_results[f\"{name} (Count)\"] = results\n",
        "\n",
        "# Seleccionar mejor modelo (mayor F1 en test, priorizando overfitting < 5%)\n",
        "comparison_all = compare_models(all_results)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SELECCI√ìN DEL MEJOR MODELO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Filtrar modelos con overfitting < 5%\n",
        "good_models = comparison_all[comparison_all['Overfitting (%)'] < 5.0]\n",
        "\n",
        "if len(good_models) > 0:\n",
        "    best_model_name = good_models.iloc[0]['Modelo']\n",
        "    best_model_results = all_results[best_model_name]\n",
        "    print(f\"\\nüèÜ MEJOR MODELO (Overfitting < 5%): {best_model_name}\")\n",
        "    print(f\"   F1-score (test): {best_model_results['test_f1']:.4f}\")\n",
        "    print(f\"   Overfitting: {best_model_results['diff_f1']:.2f}%\")\n",
        "    print(f\"   Accuracy: {best_model_results['test_accuracy']:.4f}\")\n",
        "    \n",
        "    # Determinar qu√© modelo y vectorizador usar\n",
        "    if 'TF-IDF' in best_model_name:\n",
        "        best_model = models_tfidf[best_model_name.replace(' (TF-IDF)', '')]\n",
        "        vectorizer_type = 'tfidf'\n",
        "    else:\n",
        "        best_model = models_count[best_model_name.replace(' (Count)', '')]\n",
        "        vectorizer_type = 'count'\n",
        "else:\n",
        "    # Si ning√∫n modelo cumple, elegir el mejor F1\n",
        "    best_model_name = comparison_all.iloc[0]['Modelo']\n",
        "    best_model_results = all_results[best_model_name]\n",
        "    print(f\"\\n‚ö†Ô∏è  MEJOR MODELO (Overfitting > 5%): {best_model_name}\")\n",
        "    print(f\"   F1-score (test): {best_model_results['test_f1']:.4f}\")\n",
        "    print(f\"   Overfitting: {best_model_results['diff_f1']:.2f}%\")\n",
        "    print(f\"   ‚ö†Ô∏è  Necesita optimizaci√≥n para reducir overfitting\")\n",
        "    \n",
        "    if 'TF-IDF' in best_model_name:\n",
        "        best_model = models_tfidf[best_model_name.replace(' (TF-IDF)', '')]\n",
        "        vectorizer_type = 'tfidf'\n",
        "    else:\n",
        "        best_model = models_count[best_model_name.replace(' (Count)', '')]\n",
        "        vectorizer_type = 'count'\n",
        "\n",
        "print(f\"\\nüìä Top 3 modelos:\")\n",
        "print(comparison_all.head(3).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Guardar mejor modelo baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar mejor modelo\n",
        "models_dir = Path('../models/baseline')\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model_path = models_dir / 'best_baseline_model.pkl'\n",
        "model_info = {\n",
        "    'model_name': best_model_name,\n",
        "    'vectorizer_type': vectorizer_type,\n",
        "    'test_f1': best_model_results['test_f1'],\n",
        "    'test_accuracy': best_model_results['test_accuracy'],\n",
        "    'overfitting': best_model_results['diff_f1'],\n",
        "    'train_f1': best_model_results['train_f1']\n",
        "}\n",
        "\n",
        "save_model(best_model, model_path, model_info)\n",
        "\n",
        "print(f\"\\n‚úÖ Modelo baseline guardado:\")\n",
        "print(f\"   {model_path}\")\n",
        "print(f\"   Informaci√≥n: {models_dir / 'best_baseline_model_info.pkl'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Resumen del modelado baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"RESUMEN DEL MODELADO BASELINE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n‚úÖ Modelos entrenados:\")\n",
        "print(f\"   1. Naive Bayes\")\n",
        "print(f\"   2. Logistic Regression\")\n",
        "print(f\"   3. SVM\")\n",
        "print(f\"   4. Random Forest\")\n",
        "\n",
        "print(f\"\\n‚úÖ Vectorizadores probados:\")\n",
        "print(f\"   1. TF-IDF\")\n",
        "print(f\"   2. Count Vectorizer\")\n",
        "\n",
        "print(f\"\\nüèÜ Mejor modelo seleccionado:\")\n",
        "print(f\"   {best_model_name}\")\n",
        "print(f\"   F1-score (test): {best_model_results['test_f1']:.4f}\")\n",
        "print(f\"   Overfitting: {best_model_results['diff_f1']:.2f}%\")\n",
        "\n",
        "print(f\"\\nüíæ Modelo guardado en:\")\n",
        "print(f\"   ../models/baseline/best_baseline_model.pkl\")\n",
        "\n",
        "print(\"\\n‚úÖ Modelado baseline completado\")\n",
        "print(\"   Pr√≥ximo paso: Optimizaci√≥n de hiperpar√°metros y reducci√≥n de overfitting\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
