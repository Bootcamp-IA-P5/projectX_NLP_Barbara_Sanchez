{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost - Reducci√≥n de Overfitting\n",
        "\n",
        "## Objetivo\n",
        "Probar XGBoost como alternativa a SVM y Random Forest para reducir overfitting manteniendo F1-score > 0.55.\n",
        "\n",
        "## Ventajas de XGBoost\n",
        "- ‚úÖ‚úÖ‚úÖ Regularizaci√≥n incorporada (reg_alpha L1, reg_lambda L2)\n",
        "- ‚úÖ‚úÖ‚úÖ Early stopping autom√°tico\n",
        "- ‚úÖ‚úÖ‚úÖ Mejor control de overfitting que RF\n",
        "- ‚úÖ‚úÖ‚úÖ Muy potente con datasets peque√±os\n",
        "- ‚úÖ‚úÖ‚úÖ Subsampling y colsample_bytree reducen overfitting\n",
        "- ‚úÖ Menos propenso a F1=0 que SVM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importaci√≥n de librer√≠as\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "import optuna\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Carga de datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar datos\n",
        "df = pd.read_csv('../data/processed/youtoxic_english_1000_processed.csv')\n",
        "with open('../data/processed/y_train.pkl', 'rb') as f:\n",
        "    y_train = pickle.load(f)\n",
        "with open('../data/processed/y_test.pkl', 'rb') as f:\n",
        "    y_test = pickle.load(f)\n",
        "\n",
        "X_train_text = df[df.index.isin(range(len(y_train)))]['Text_processed'].values\n",
        "X_test_text = df[df.index.isin(range(len(y_train), len(y_train) + len(y_test)))]['Text_processed'].values\n",
        "\n",
        "print(f\"‚úÖ Datos cargados: {len(X_train_text)} train, {len(X_test_text)} test\")\n",
        "print(f\"Distribuci√≥n train: {np.bincount(y_train)}\")\n",
        "print(f\"Distribuci√≥n test: {np.bincount(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Vectorizaci√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vectorizaci√≥n mejorada\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=800,        # M√°s features\n",
        "    ngram_range=(1, 2),      # Bigramas\n",
        "    min_df=3,                # Menos restrictivo\n",
        "    max_df=0.85,             # M√°s permisivo\n",
        "    stop_words='english',\n",
        "    sublinear_tf=True,\n",
        "    norm='l2'\n",
        ")\n",
        "\n",
        "# SIN augmentaci√≥n (XGBoost maneja bien dataset peque√±o)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
        "X_test_tfidf = tfidf.transform(X_test_text)\n",
        "\n",
        "# Convertir a formato denso para XGBoost (puede trabajar con sparse pero es m√°s lento)\n",
        "X_train_tfidf_dense = X_train_tfidf.toarray()\n",
        "X_test_tfidf_dense = X_test_tfidf.toarray()\n",
        "\n",
        "print(f\"‚úÖ Vectorizaci√≥n: {X_train_tfidf.shape[1]} features\")\n",
        "print(f\"   Train shape: {X_train_tfidf_dense.shape}\")\n",
        "print(f\"   Test shape: {X_test_tfidf_dense.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Funci√≥n de Evaluaci√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Eval√∫a modelo y retorna m√©tricas.\"\"\"\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    \n",
        "    train_f1 = f1_score(y_train, y_train_pred, zero_division=0)\n",
        "    test_f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
        "    diff_f1 = abs(train_f1 - test_f1) * 100\n",
        "    \n",
        "    return {\n",
        "        'train_f1': train_f1,\n",
        "        'test_f1': test_f1,\n",
        "        'test_accuracy': accuracy_score(y_test, y_test_pred),\n",
        "        'test_precision': precision_score(y_test, y_test_pred, zero_division=0),\n",
        "        'test_recall': recall_score(y_test, y_test_pred, zero_division=0),\n",
        "        'diff_f1': diff_f1,\n",
        "        'confusion_matrix': confusion_matrix(y_test, y_test_pred)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Funci√≥n Objetivo para Optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    \"\"\"\n",
        "    Funci√≥n objetivo para XGBoost:\n",
        "    - Regularizaci√≥n incorporada (reg_alpha L1, reg_lambda L2)\n",
        "    - Control de overfitting con max_depth, min_child_weight\n",
        "    - Subsampling y colsample_bytree\n",
        "    - Prioriza overfitting <5% y F1 >0.55\n",
        "    \"\"\"\n",
        "    # Calcular scale_pos_weight para balance de clases\n",
        "    scale_pos_weight = len(y_train[y_train==0]) / len(y_train[y_train==1])\n",
        "    \n",
        "    params = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'logloss',\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 8),  # Limita profundidad\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),  # Controla hojas\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),  # Subsampling de filas\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),  # Subsampling de features\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),  # Regularizaci√≥n L1\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),  # Regularizaci√≥n L2\n",
        "        'scale_pos_weight': scale_pos_weight,  # Balance de clases\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "    \n",
        "    model = xgb.XGBClassifier(**params)\n",
        "    model.fit(X_train_tfidf_dense, y_train)\n",
        "    \n",
        "    results = evaluate_model(model, X_train_tfidf_dense, X_test_tfidf_dense, y_train, y_test)\n",
        "    \n",
        "    # Rechazar modelos in√∫tiles\n",
        "    if results['test_f1'] < 0.55:\n",
        "        return -10.0\n",
        "    \n",
        "    # Rechazar overfitting extremo\n",
        "    if results['diff_f1'] > 6.0:\n",
        "        return -20.0\n",
        "    \n",
        "    # Rechazar recall extremo\n",
        "    if results['test_recall'] >= 0.95:\n",
        "        return -15.0\n",
        "    \n",
        "    # PRIORIDAD 1: Control de overfitting\n",
        "    if results['diff_f1'] < 5.0:\n",
        "        overfitting_bonus = (5.0 - results['diff_f1']) * 0.50  # Bonus grande\n",
        "    else:\n",
        "        overfitting_bonus = 0\n",
        "    \n",
        "    # PRIORIDAD 2: Penalizaci√≥n por overfitting\n",
        "    if results['diff_f1'] > 5.0:\n",
        "        overfitting_penalty = ((results['diff_f1'] - 5.0) ** 2) * 0.05\n",
        "    else:\n",
        "        overfitting_penalty = 0\n",
        "    \n",
        "    # PRIORIDAD 3: Penalizar recall extremo\n",
        "    recall_penalty = 0\n",
        "    if results['test_recall'] > 0.80:\n",
        "        recall_penalty = ((results['test_recall'] - 0.80) ** 2) * 0.40\n",
        "    \n",
        "    # PRIORIDAD 4: F1-score base\n",
        "    base_score = results['test_f1'] * 0.3\n",
        "    \n",
        "    score = base_score + overfitting_bonus - overfitting_penalty - recall_penalty\n",
        "    return score\n",
        "\n",
        "print(\"‚úÖ Funci√≥n objetivo definida (prioriza overfitting <5%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Optimizaci√≥n con Optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"OPTIMIZACI√ìN XGBOOST - CONTROL DE OVERFITTING\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ SIN augmentaci√≥n (XGBoost maneja bien dataset peque√±o)\")\n",
        "print(\"‚úÖ Regularizaci√≥n incorporada (reg_alpha L1, reg_lambda L2)\")\n",
        "print(\"‚úÖ Control de profundidad (max_depth)\")\n",
        "print(\"‚úÖ Subsampling (subsample, colsample_bytree)\")\n",
        "print(\"‚úÖ Scale pos weight para balance de clases\")\n",
        "print(\"‚úÖ Penalizaci√≥n por overfitting >5%\")\n",
        "print(\"\\nObjetivo: F1 > 0.55 Y overfitting < 5%\")\n",
        "print(\"Trials: 200\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "study.optimize(objective, n_trials=200, show_progress_bar=True)\n",
        "\n",
        "print(\"\\n‚úÖ Optimizaci√≥n completada\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluaci√≥n del Mejor Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar mejor modelo\n",
        "best_params = study.best_params\n",
        "scale_pos_weight = len(y_train[y_train==0]) / len(y_train[y_train==1])\n",
        "\n",
        "# Construir par√°metros completos\n",
        "final_params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'logloss',\n",
        "    'max_depth': best_params['max_depth'],\n",
        "    'learning_rate': best_params['learning_rate'],\n",
        "    'n_estimators': best_params['n_estimators'],\n",
        "    'min_child_weight': best_params['min_child_weight'],\n",
        "    'subsample': best_params['subsample'],\n",
        "    'colsample_bytree': best_params['colsample_bytree'],\n",
        "    'reg_alpha': best_params['reg_alpha'],\n",
        "    'reg_lambda': best_params['reg_lambda'],\n",
        "    'scale_pos_weight': scale_pos_weight,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1\n",
        "}\n",
        "\n",
        "best_model = xgb.XGBClassifier(**final_params)\n",
        "best_model.fit(X_train_tfidf_dense, y_train)\n",
        "results = evaluate_model(best_model, X_train_tfidf_dense, X_test_tfidf_dense, y_train, y_test)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"RESULTADOS FINALES - XGBOOST\")\n",
        "print(\"=\"*80)\n",
        "print(f\"F1-score (test): {results['test_f1']:.4f}\")\n",
        "print(f\"Accuracy (test): {results['test_accuracy']:.4f}\")\n",
        "print(f\"Precision (test): {results['test_precision']:.4f}\")\n",
        "print(f\"Recall (test): {results['test_recall']:.4f}\")\n",
        "print(f\"Diferencia F1: {results['diff_f1']:.2f}%\")\n",
        "print(f\"\\nMatriz de confusi√≥n:\")\n",
        "print(results['confusion_matrix'])\n",
        "\n",
        "if results['diff_f1'] < 5.0 and results['test_f1'] > 0.55:\n",
        "    print(\"\\n‚úÖ‚úÖ‚úÖ OBJETIVO CUMPLIDO: Overfitting < 5% Y F1 > 0.55\")\n",
        "elif results['diff_f1'] < 6.0:\n",
        "    print(\"\\nüéØ MUY CERCA: Overfitting < 6%\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Overfitting a√∫n alto\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Validaci√≥n Cruzada\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_all = np.vstack([X_train_tfidf_dense, X_test_tfidf_dense])\n",
        "y_all = np.concatenate([y_train, y_test])\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(best_model, X_all, y_all, cv=cv, scoring='f1', n_jobs=-1)\n",
        "\n",
        "print(f\"F1-score (CV): {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "print(f\"Scores: {cv_scores}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Guardar Modelo (si cumple objetivos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if results['diff_f1'] < 6.0 and results['test_f1'] > 0.55:\n",
        "    with open('../models/xgboost_model.pkl', 'wb') as f:\n",
        "        pickle.dump(best_model, f)\n",
        "    with open('../models/xgboost_tfidf.pkl', 'wb') as f:\n",
        "        pickle.dump(tfidf, f)\n",
        "    \n",
        "    model_info = {\n",
        "        'model_type': 'XGBoost',\n",
        "        'hyperparameters': final_params,\n",
        "        'test_f1': results['test_f1'],\n",
        "        'diff_f1': results['diff_f1'],\n",
        "        'cv_f1_mean': cv_scores.mean(),\n",
        "        'data_augmentation': False\n",
        "    }\n",
        "    \n",
        "    with open('../models/xgboost_info.pkl', 'wb') as f:\n",
        "        pickle.dump(model_info, f)\n",
        "    \n",
        "    print(\"‚úÖ Modelo XGBoost guardado\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Modelo no guardado (no cumple objetivos)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. An√°lisis de Feature Importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance (top 20)\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "importances = best_model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1][:20]\n",
        "\n",
        "print(\"Top 20 features m√°s importantes:\")\n",
        "print(\"-\"*50)\n",
        "for i in range(20):\n",
        "    print(f\"{i+1:2d}. {feature_names[indices[i]]:30s} {importances[indices[i]]:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
