{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“Š MLFlow Tracking - Experimentos de Hate Speech Detection\n",
        "\n",
        "Este notebook demuestra cÃ³mo usar MLFlow para trackear experimentos de machine learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# AÃ±adir src al path\n",
        "project_root = Path('../').resolve()\n",
        "sys.path.append(str(project_root / 'src'))\n",
        "\n",
        "from models.train import train_model\n",
        "from models.evaluate import evaluate_model\n",
        "from features.vectorization import load_vectorized_data\n",
        "from utils.mlflow_tracking import get_tracker\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Cargar Datos Vectorizados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar datos vectorizados\n",
        "X_train, X_test, y_train, y_test = load_vectorized_data(\n",
        "    input_dir=Path('../data/processed'),\n",
        "    prefix='tfidf'\n",
        ")\n",
        "\n",
        "print(f\"âœ… Datos cargados:\")\n",
        "print(f\"   Train: {X_train.shape}\")\n",
        "print(f\"   Test: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Inicializar MLFlow Tracker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicializar tracker de MLFlow\n",
        "tracker = get_tracker(experiment_name=\"hate_speech_detection\")\n",
        "print(f\"âœ… MLFlow tracker inicializado: {tracker.experiment_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Entrenar y Registrar Modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar mÃºltiples modelos y registrarlos en MLFlow\n",
        "models_to_test = [\n",
        "    {'name': 'svm', 'type': 'svm', 'params': {'C': 0.056, 'kernel': 'linear', 'class_weight': 'balanced'}},\n",
        "    {'name': 'logistic', 'type': 'logistic', 'params': {'C': 0.1, 'penalty': 'l2', 'class_weight': 'balanced', 'max_iter': 1000}},\n",
        "    {'name': 'naive_bayes', 'type': 'naive_bayes', 'params': {'alpha': 10.0}},\n",
        "    {'name': 'random_forest', 'type': 'random_forest', 'params': {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 5, 'class_weight': 'balanced'}}\n",
        "]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_config in models_to_test:\n",
        "    print(f\"\\nðŸ”§ Entrenando {model_config['name']}...\")\n",
        "    \n",
        "    # Entrenar modelo\n",
        "    model = train_model(\n",
        "        X_train, y_train,\n",
        "        model_type=model_config['type'],\n",
        "        **model_config['params']\n",
        "    )\n",
        "    \n",
        "    # Evaluar modelo\n",
        "    metrics = evaluate_model(\n",
        "        model, X_train, X_test, y_train, y_test, verbose=False\n",
        "    )\n",
        "    \n",
        "    results[model_config['name']] = metrics\n",
        "    \n",
        "    # Registrar en MLFlow\n",
        "    tracker.log_model_training(\n",
        "        model=model,\n",
        "        model_name=model_config['name'],\n",
        "        metrics=metrics,\n",
        "        params=model_config['params'],\n",
        "        vectorizer_type='tfidf',\n",
        "        tags={'experiment': 'model_comparison', 'vectorizer': 'tfidf'}\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ… {model_config['name']} registrado en MLFlow\")\n",
        "    print(f\"   F1-score (test): {metrics['test_f1']:.4f}\")\n",
        "    print(f\"   Overfitting: {metrics['diff_f1']:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear DataFrame con resultados\n",
        "comparison_data = []\n",
        "for model_name, metrics in results.items():\n",
        "    comparison_data.append({\n",
        "        'Modelo': model_name,\n",
        "        'F1 (test)': metrics['test_f1'],\n",
        "        'F1 (train)': metrics['train_f1'],\n",
        "        'Overfitting (%)': metrics['diff_f1'],\n",
        "        'Accuracy (test)': metrics['test_accuracy'],\n",
        "        'Precision (test)': metrics['test_precision'],\n",
        "        'Recall (test)': metrics['test_recall']\n",
        "    })\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "df_comparison = df_comparison.sort_values('F1 (test)', ascending=False)\n",
        "\n",
        "print(\"\\nðŸ“Š ComparaciÃ³n de Modelos:\")\n",
        "print(df_comparison.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualizar en MLFlow UI\n",
        "\n",
        "Para ver los experimentos en la interfaz de MLFlow:\n",
        "\n",
        "```bash\n",
        "mlflow ui\n",
        "```\n",
        "\n",
        "Luego abre: http://localhost:5000\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
