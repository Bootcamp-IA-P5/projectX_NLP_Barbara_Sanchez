{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Text Classification - Reducci√≥n de Overfitting\n",
    "\n",
    "## Objetivo\n",
    "Probar spaCy TextCategorizer como alternativa a modelos cl√°sicos para reducir overfitting manteniendo F1-score > 0.55.\n",
    "\n",
    "## Ventajas de spaCy Text Classification\n",
    "- ‚úÖ‚úÖ‚úÖ Word embeddings pre-entrenados (mejor sem√°ntica que TF-IDF)\n",
    "- ‚úÖ‚úÖ‚úÖ Modelo espec√≠fico para clasificaci√≥n de texto\n",
    "- ‚úÖ‚úÖ‚úÖ Fine-tuning con tu dataset\n",
    "- ‚úÖ‚úÖ‚úÖ Mejor control de overfitting que modelos cl√°sicos\n",
    "- ‚úÖ‚úÖ‚úÖ M√°s r√°pido que Transformers\n",
    "- ‚úÖ Ya tienes spaCy instalado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importaci√≥n de librer√≠as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as importadas\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar modelo de spaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Modelos con word vectors no encontrados\n",
      "   Usando en_core_web_sm (sin word vectors)\n",
      "   Para mejor rendimiento, instala: python -m spacy download en_core_web_md\n"
     ]
    }
   ],
   "source": [
    "# Intentar cargar modelo con word vectors (md o lg)\n",
    "# Si no est√° disponible, usar sm y descargar md\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    print(\"‚úÖ Modelo en_core_web_md cargado (con word vectors)\")\n",
    "except OSError:\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_lg')\n",
    "        print(\"‚úÖ Modelo en_core_web_lg cargado (con word vectors)\")\n",
    "    except OSError:\n",
    "        print(\"‚ö†Ô∏è  Modelos con word vectors no encontrados\")\n",
    "        print(\"   Usando en_core_web_sm (sin word vectors)\")\n",
    "        print(\"   Para mejor rendimiento, instala: python -m spacy download en_core_web_md\")\n",
    "        nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos cargados: 800 train, 200 test\n",
      "Distribuci√≥n train: [430 370]\n",
      "Distribuci√≥n test: [108  92]\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "df = pd.read_csv('../data/processed/youtoxic_english_1000_processed.csv')\n",
    "with open('../data/processed/y_train.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open('../data/processed/y_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "X_train_text = df[df.index.isin(range(len(y_train)))]['Text_processed'].values\n",
    "X_test_text = df[df.index.isin(range(len(y_train), len(y_train) + len(y_test)))]['Text_processed'].values\n",
    "\n",
    "print(f\"‚úÖ Datos cargados: {len(X_train_text)} train, {len(X_test_text)} test\")\n",
    "print(f\"Distribuci√≥n train: {np.bincount(y_train)}\")\n",
    "print(f\"Distribuci√≥n test: {np.bincount(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparar datos en formato spaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos preparados: 800 train, 200 test\n",
      "Ejemplo de formato:\n",
      "  Texto: people would take step back make case be not anyon...\n",
      "  Categor√≠as: {'cats': {'TOXIC': 1.0, 'NOT_TOXIC': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "# Preparar datos en formato spaCy\n",
    "# spaCy espera: (texto, {\"cats\": {\"TOXIC\": 1.0, \"NOT_TOXIC\": 0.0}})\n",
    "\n",
    "def prepare_spacy_data(texts, labels):\n",
    "    \"\"\"Prepara datos en formato spaCy para TextCategorizer.\"\"\"\n",
    "    data = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        # Convertir label a categor√≠as\n",
    "        cats = {\n",
    "            \"TOXIC\": 1.0 if label == 1 else 0.0,\n",
    "            \"NOT_TOXIC\": 1.0 if label == 0 else 0.0\n",
    "        }\n",
    "        data.append((text, {\"cats\": cats}))\n",
    "    return data\n",
    "\n",
    "# Preparar datos de entrenamiento y prueba\n",
    "train_data = prepare_spacy_data(X_train_text, y_train)\n",
    "test_data = prepare_spacy_data(X_test_text, y_test)\n",
    "\n",
    "print(f\"‚úÖ Datos preparados: {len(train_data)} train, {len(test_data)} test\")\n",
    "print(f\"Ejemplo de formato:\")\n",
    "print(f\"  Texto: {train_data[0][0][:50]}...\")\n",
    "print(f\"  Categor√≠as: {train_data[0][1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Crear y configurar TextCategorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TextCategorizer configurado\n",
      "Etiquetas: ('TOXIC', 'NOT_TOXIC')\n"
     ]
    }
   ],
   "source": [
    "# Crear pipeline de spaCy con TextCategorizer\n",
    "if \"textcat\" not in nlp.pipe_names:\n",
    "    # A√±adir TextCategorizer al pipeline\n",
    "    textcat = nlp.add_pipe(\"textcat\", last=True)\n",
    "else:\n",
    "    textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "# A√±adir etiquetas\n",
    "textcat.add_label(\"TOXIC\")\n",
    "textcat.add_label(\"NOT_TOXIC\")\n",
    "\n",
    "print(\"‚úÖ TextCategorizer configurado\")\n",
    "print(f\"Etiquetas: {textcat.labels}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Funci√≥n de evaluaci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n de evaluaci√≥n definida\n"
     ]
    }
   ],
   "source": [
    "def evaluate_spacy_model(nlp_model, texts, true_labels):\n",
    "    \"\"\"Eval√∫a modelo de spaCy y retorna m√©tricas.\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    # Procesar textos y obtener predicciones\n",
    "    for text in texts:\n",
    "        doc = nlp_model(text)\n",
    "        # Obtener probabilidad de clase TOXIC\n",
    "        toxic_score = doc.cats.get(\"TOXIC\", 0.0)\n",
    "        # Predicci√≥n: 1 si TOXIC > 0.5, 0 en caso contrario\n",
    "        pred = 1 if toxic_score > 0.5 else 0\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
    "    f1 = f1_score(true_labels, predictions, zero_division=0)\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': predictions\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de evaluaci√≥n definida\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entrenar modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENTRENAMIENTO SPACY TEXTCATEGORIZER\n",
      "================================================================================\n",
      "Componentes deshabilitados: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "√âpocas: 10\n",
      "Batch size: compuesto (4-32)\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö†Ô∏è  Advertencia: Problemas con lookups, inicializando sin ellos...\n",
      "‚ö†Ô∏è  Usando m√©todo alternativo de inicializaci√≥n...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E955] Can't find table(s) lexeme_norm for language 'en' in spacy-lookups-data. Make sure you have the package installed or provide your own lookup tables if no default lookups are available for your language.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43mnlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/spacy/language.py:1326\u001b[39m, in \u001b[36mLanguage.initialize\u001b[39m\u001b[34m(self, get_examples, sgd)\u001b[39m\n\u001b[32m   1325\u001b[39m \u001b[38;5;66;03m# These are the settings provided in the [initialize] block in the config\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m I = \u001b[43mregistry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minitialize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mConfigSchemaInit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1327\u001b[39m before_init = I[\u001b[33m\"\u001b[39m\u001b[33mbefore_init\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/confection/__init__.py:760\u001b[39m, in \u001b[36mregistry.resolve\u001b[39m\u001b[34m(cls, config, schema, overrides, validate)\u001b[39m\n\u001b[32m    751\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresolve\u001b[39m(\n\u001b[32m    753\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    758\u001b[39m     validate: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    759\u001b[39m ) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m--> \u001b[39m\u001b[32m760\u001b[39m     resolved, _ = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolve\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    762\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    763\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/confection/__init__.py:809\u001b[39m, in \u001b[36mregistry._make\u001b[39m\u001b[34m(cls, config, schema, overrides, resolve, validate)\u001b[39m\n\u001b[32m    808\u001b[39m     config = Config(orig_config).interpolate()\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m filled, _, resolved = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fill\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolve\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresolve\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m filled = Config(filled, section_order=section_order)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/confection/__init__.py:881\u001b[39m, in \u001b[36mregistry._fill\u001b[39m\u001b[34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[39m\n\u001b[32m    879\u001b[39m     \u001b[38;5;66;03m# We don't want to try/except this and raise our own error\u001b[39;00m\n\u001b[32m    880\u001b[39m     \u001b[38;5;66;03m# here, because we want the traceback if the function fails.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m881\u001b[39m     getter_result = \u001b[43mgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    883\u001b[39m     \u001b[38;5;66;03m# We're not resolving and calling the function, so replace\u001b[39;00m\n\u001b[32m    884\u001b[39m     \u001b[38;5;66;03m# the getter_result with a Promise class\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/spacy/language.py:134\u001b[39m, in \u001b[36mload_lookups_data\u001b[39m\u001b[34m(lang, tables)\u001b[39m\n\u001b[32m    133\u001b[39m util.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLoading lookups from spacy-lookups-data: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, tables)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m lookups = \u001b[43mload_lookups\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m lookups\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/spacy/lookups.py:30\u001b[39m, in \u001b[36mload_lookups\u001b[39m\u001b[34m(lang, tables, strict)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tables) > \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E955.format(table=\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(tables), lang=lang))\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m lookups\n",
      "\u001b[31mValueError\u001b[39m: [E955] Can't find table(s) lexeme_norm for language 'en' in spacy-lookups-data. Make sure you have the package installed or provide your own lookup tables if no default lookups are available for your language.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[43mnlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlookups\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# Si a√∫n falla, usar m√©todo alternativo\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: Language.initialize() got an unexpected keyword argument 'exclude'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     28\u001b[39m         \u001b[38;5;66;03m# Si a√∫n falla, usar m√©todo alternativo\u001b[39;00m\n\u001b[32m     29\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è  Usando m√©todo alternativo de inicializaci√≥n...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m         \u001b[43mnlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/spacy/language.py:1326\u001b[39m, in \u001b[36mLanguage.initialize\u001b[39m\u001b[34m(self, get_examples, sgd)\u001b[39m\n\u001b[32m   1324\u001b[39m config = \u001b[38;5;28mself\u001b[39m.config.interpolate()\n\u001b[32m   1325\u001b[39m \u001b[38;5;66;03m# These are the settings provided in the [initialize] block in the config\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m I = \u001b[43mregistry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minitialize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mConfigSchemaInit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1327\u001b[39m before_init = I[\u001b[33m\"\u001b[39m\u001b[33mbefore_init\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m before_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/confection/__init__.py:760\u001b[39m, in \u001b[36mregistry.resolve\u001b[39m\u001b[34m(cls, config, schema, overrides, validate)\u001b[39m\n\u001b[32m    751\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresolve\u001b[39m(\n\u001b[32m    753\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    758\u001b[39m     validate: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    759\u001b[39m ) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m--> \u001b[39m\u001b[32m760\u001b[39m     resolved, _ = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolve\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    762\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    763\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/confection/__init__.py:809\u001b[39m, in \u001b[36mregistry._make\u001b[39m\u001b[34m(cls, config, schema, overrides, resolve, validate)\u001b[39m\n\u001b[32m    807\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interpolated:\n\u001b[32m    808\u001b[39m     config = Config(orig_config).interpolate()\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m filled, _, resolved = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fill\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolve\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresolve\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m filled = Config(filled, section_order=section_order)\n\u001b[32m    813\u001b[39m \u001b[38;5;66;03m# Check that overrides didn't include invalid properties not in config\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/confection/__init__.py:881\u001b[39m, in \u001b[36mregistry._fill\u001b[39m\u001b[34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[39m\n\u001b[32m    878\u001b[39m     getter = \u001b[38;5;28mcls\u001b[39m.get(reg_name, func_name)\n\u001b[32m    879\u001b[39m     \u001b[38;5;66;03m# We don't want to try/except this and raise our own error\u001b[39;00m\n\u001b[32m    880\u001b[39m     \u001b[38;5;66;03m# here, because we want the traceback if the function fails.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m881\u001b[39m     getter_result = \u001b[43mgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    883\u001b[39m     \u001b[38;5;66;03m# We're not resolving and calling the function, so replace\u001b[39;00m\n\u001b[32m    884\u001b[39m     \u001b[38;5;66;03m# the getter_result with a Promise class\u001b[39;00m\n\u001b[32m    885\u001b[39m     getter_result = Promise(\n\u001b[32m    886\u001b[39m         registry=reg_name, name=func_name, args=args, kwargs=kwargs\n\u001b[32m    887\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/spacy/language.py:134\u001b[39m, in \u001b[36mload_lookups_data\u001b[39m\u001b[34m(lang, tables)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lookups_data\u001b[39m(lang, tables):\n\u001b[32m    133\u001b[39m     util.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLoading lookups from spacy-lookups-data: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, tables)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     lookups = \u001b[43mload_lookups\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lookups\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/spacy/lookups.py:30\u001b[39m, in \u001b[36mload_lookups\u001b[39m\u001b[34m(lang, tables, strict)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m registry.lookups:\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tables) > \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E955.format(table=\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(tables), lang=lang))\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lookups\n\u001b[32m     32\u001b[39m data = registry.lookups.get(lang)\n",
      "\u001b[31mValueError\u001b[39m: [E955] Can't find table(s) lexeme_norm for language 'en' in spacy-lookups-data. Make sure you have the package installed or provide your own lookup tables if no default lookups are available for your language."
     ]
    }
   ],
   "source": [
    "# Convertir datos a formato Example de spaCy\n",
    "train_examples = [Example.from_dict(nlp.make_doc(text), annots) for text, annots in train_data]\n",
    "\n",
    "# Deshabilitar otros componentes del pipeline durante entrenamiento\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ENTRENAMIENTO SPACY TEXTCATEGORIZER\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Componentes deshabilitados: {other_pipes}\")\n",
    "print(f\"√âpocas: 10\")\n",
    "print(f\"Batch size: compuesto (4-32)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Entrenar modelo\n",
    "n_iter = 10\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    # Inicializar el componente\n",
    "    # Si hay problemas con lookups, intentar sin ellos\n",
    "    try:\n",
    "        nlp.initialize(lambda: train_examples)\n",
    "    except ValueError as e:\n",
    "        if \"lookups\" in str(e).lower() or \"lookup\" in str(e).lower():\n",
    "            print(\"‚ö†Ô∏è  Advertencia: Problemas con lookups, inicializando sin ellos...\")\n",
    "            try:\n",
    "                nlp.initialize(lambda: train_examples, exclude=[\"lookups\"])\n",
    "            except:\n",
    "                # Si a√∫n falla, usar m√©todo alternativo\n",
    "                print(\"‚ö†Ô∏è  Usando m√©todo alternativo de inicializaci√≥n...\")\n",
    "                nlp.initialize(get_examples=lambda: train_examples)\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    # Entrenar\n",
    "    for epoch in range(n_iter):\n",
    "        # Mezclar datos\n",
    "        random.shuffle(train_examples)\n",
    "        \n",
    "        # Crear batches de tama√±o variable\n",
    "        batches = minibatch(train_examples, size=compounding(4.0, 32.0, 1.001))\n",
    "        \n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            nlp.update(batch, losses=losses, drop=0.2)  # drop=0.2 para regularizaci√≥n\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"√âpoca {epoch+1}/{n_iter} - Loss: {losses.get('textcat', 0.0):.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Entrenamiento completado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluaci√≥n del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar en train y test\n",
    "print(\"Evaluando en conjunto de entrenamiento...\")\n",
    "train_results = evaluate_spacy_model(nlp, X_train_text, y_train)\n",
    "\n",
    "print(\"\\nEvaluando en conjunto de prueba...\")\n",
    "test_results = evaluate_spacy_model(nlp, X_test_text, y_test)\n",
    "\n",
    "# Calcular diferencia de F1 (overfitting)\n",
    "diff_f1 = abs(train_results['f1'] - test_results['f1']) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTADOS FINALES - SPACY TEXTCATEGORIZER\")\n",
    "print(\"=\"*80)\n",
    "print(f\"F1-score (train): {train_results['f1']:.4f}\")\n",
    "print(f\"F1-score (test): {test_results['f1']:.4f}\")\n",
    "print(f\"Accuracy (test): {test_results['accuracy']:.4f}\")\n",
    "print(f\"Precision (test): {test_results['precision']:.4f}\")\n",
    "print(f\"Recall (test): {test_results['recall']:.4f}\")\n",
    "print(f\"Diferencia F1: {diff_f1:.2f}%\")\n",
    "print(f\"\\nMatriz de confusi√≥n (test):\")\n",
    "print(test_results['confusion_matrix'])\n",
    "\n",
    "if diff_f1 < 5.0 and test_results['f1'] > 0.55:\n",
    "    print(\"\\n‚úÖ‚úÖ‚úÖ OBJETIVO CUMPLIDO: Overfitting < 5% Y F1 > 0.55\")\n",
    "elif diff_f1 < 6.0:\n",
    "    print(\"\\nüéØ MUY CERCA: Overfitting < 6%\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Overfitting a√∫n alto\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validaci√≥n Cruzada (opcional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validaci√≥n cruzada (toma tiempo, opcional)\n",
    "print(\"Validaci√≥n cruzada (5-fold)...\")\n",
    "print(\"‚ö†Ô∏è  Esto puede tardar varios minutos...\")\n",
    "\n",
    "X_all_text = np.concatenate([X_train_text, X_test_text])\n",
    "y_all = np.concatenate([y_train, y_test])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_all_text, y_all), 1):\n",
    "    print(f\"Fold {fold}/5...\")\n",
    "    \n",
    "    # Preparar datos del fold\n",
    "    fold_train_data = prepare_spacy_data(X_all_text[train_idx], y_all[train_idx])\n",
    "    fold_val_data = prepare_spacy_data(X_all_text[val_idx], y_all[val_idx])\n",
    "    \n",
    "    # Crear nuevo modelo para este fold\n",
    "    fold_nlp = spacy.load('en_core_web_md' if 'md' in str(nlp.meta.get('name', '')) else 'en_core_web_sm')\n",
    "    if \"textcat\" not in fold_nlp.pipe_names:\n",
    "        fold_textcat = fold_nlp.add_pipe(\"textcat\", last=True)\n",
    "    else:\n",
    "        fold_textcat = fold_nlp.get_pipe(\"textcat\")\n",
    "    fold_textcat.add_label(\"TOXIC\")\n",
    "    fold_textcat.add_label(\"NOT_TOXIC\")\n",
    "    \n",
    "    # Entrenar\n",
    "    fold_examples = [Example.from_dict(fold_nlp.make_doc(text), annots) for text, annots in fold_train_data]\n",
    "    fold_other_pipes = [pipe for pipe in fold_nlp.pipe_names if pipe != \"textcat\"]\n",
    "    \n",
    "    with fold_nlp.disable_pipes(*fold_other_pipes):\n",
    "        fold_nlp.initialize(lambda: fold_examples)\n",
    "        for epoch in range(5):  # Menos √©pocas para CV\n",
    "            random.shuffle(fold_examples)\n",
    "            batches = minibatch(fold_examples, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                fold_nlp.update(batch, losses={}, drop=0.2)\n",
    "    \n",
    "    # Evaluar\n",
    "    fold_results = evaluate_spacy_model(fold_nlp, X_all_text[val_idx], y_all[val_idx])\n",
    "    cv_scores.append(fold_results['f1'])\n",
    "    print(f\"  F1-score: {fold_results['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nF1-score (CV): {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores) * 2:.4f})\")\n",
    "print(f\"Scores: {cv_scores}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Guardar modelo (si cumple objetivos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if diff_f1 < 6.0 and test_results['f1'] > 0.55:\n",
    "    # Guardar modelo\n",
    "    output_dir = Path('../models/spacy_textcat_model')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    nlp.to_disk(output_dir)\n",
    "    \n",
    "    # Guardar informaci√≥n del modelo\n",
    "    model_info = {\n",
    "        'model_type': 'spaCy TextCategorizer',\n",
    "        'spacy_model': str(nlp.meta.get('name', 'unknown')),\n",
    "        'test_f1': test_results['f1'],\n",
    "        'diff_f1': diff_f1,\n",
    "        'n_iter': n_iter,\n",
    "        'has_word_vectors': hasattr(nlp.vocab.vectors, 'shape')\n",
    "    }\n",
    "    \n",
    "    with open('../models/spacy_textcat_info.pkl', 'wb') as f:\n",
    "        pickle.dump(model_info, f)\n",
    "    \n",
    "    print(f\"‚úÖ Modelo guardado en: {output_dir}\")\n",
    "    print(f\"‚úÖ Informaci√≥n guardada en: ../models/spacy_textcat_info.pkl\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Modelo no guardado (no cumple objetivos)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
