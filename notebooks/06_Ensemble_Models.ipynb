{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Ensemble de Modelos\n",
        "## Combinaci√≥n de modelos para mejorar rendimiento\n",
        "\n",
        "### Objetivos:\n",
        "1. Implementar Voting Classifier\n",
        "2. Implementar Stacking Classifier\n",
        "3. Comparar ensemble vs modelos individuales\n",
        "4. Evaluar si mejora el rendimiento\n",
        "5. Seleccionar mejor ensemble\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importar librer√≠as\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# A√±adir src al path\n",
        "sys.path.append(str(Path('../src').resolve()))\n",
        "\n",
        "from models.ensemble import create_voting_classifier, create_stacking_classifier, compare_ensemble_vs_individual\n",
        "from models.train import train_model, save_model\n",
        "from models.evaluate import evaluate_model\n",
        "from features.vectorization import load_vectorized_data\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cargar datos vectorizados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar datos vectorizados con TF-IDF\n",
        "data_dir = Path('../data/processed')\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = load_vectorized_data(data_dir, prefix='tfidf')\n",
        "\n",
        "print(f\"‚úÖ Datos cargados:\")\n",
        "print(f\"   Train: {X_train_tfidf.shape}\")\n",
        "print(f\"   Test: {X_test_tfidf.shape}\")\n",
        "print(f\"   Labels - Train: {len(y_train)}, Test: {len(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Entrenar modelos individuales para el ensemble\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"ENTRENANDO MODELOS INDIVIDUALES PARA ENSEMBLE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Entrenar los mejores modelos del baseline\n",
        "individual_models = {}\n",
        "\n",
        "# 1. SVM (mejor modelo optimizado)\n",
        "print(\"\\nüîµ Entrenando SVM...\")\n",
        "individual_models['SVM'] = train_model(\n",
        "    'svm', X_train_tfidf, y_train,\n",
        "    C=0.1, kernel='linear', class_weight='balanced'\n",
        ")\n",
        "\n",
        "# 2. Logistic Regression\n",
        "print(\"\\nüü¢ Entrenando Logistic Regression...\")\n",
        "individual_models['Logistic Regression'] = train_model(\n",
        "    'logistic', X_train_tfidf, y_train,\n",
        "    C=1.0, penalty='l2', class_weight='balanced'\n",
        ")\n",
        "\n",
        "# 3. Naive Bayes\n",
        "print(\"\\nüü° Entrenando Naive Bayes...\")\n",
        "individual_models['Naive Bayes'] = train_model(\n",
        "    'naive_bayes', X_train_tfidf, y_train,\n",
        "    alpha=1.0\n",
        ")\n",
        "\n",
        "# 4. Random Forest\n",
        "print(\"\\nüî¥ Entrenando Random Forest...\")\n",
        "individual_models['Random Forest'] = train_model(\n",
        "    'random_forest', X_train_tfidf, y_train,\n",
        "    n_estimators=100, max_depth=10, class_weight='balanced'\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ {len(individual_models)} modelos entrenados\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Crear Voting Classifier (Soft Voting)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"CREANDO VOTING CLASSIFIER (SOFT VOTING)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Configuraci√≥n de modelos para el ensemble\n",
        "models_config = [\n",
        "    {'name': 'svm', 'type': 'svm', 'params': {'C': 0.1, 'kernel': 'linear', 'class_weight': 'balanced'}},\n",
        "    {'name': 'logistic', 'type': 'logistic', 'params': {'C': 1.0, 'penalty': 'l2', 'class_weight': 'balanced'}},\n",
        "    {'name': 'naive_bayes', 'type': 'naive_bayes', 'params': {'alpha': 1.0}},\n",
        "    {'name': 'random_forest', 'type': 'random_forest', 'params': {'n_estimators': 100, 'max_depth': 10, 'class_weight': 'balanced'}}\n",
        "]\n",
        "\n",
        "# Crear Voting Classifier con soft voting\n",
        "voting_clf = create_voting_classifier(\n",
        "    models_config,\n",
        "    X_train_tfidf,\n",
        "    y_train,\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# Evaluar\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUACI√ìN VOTING CLASSIFIER\")\n",
        "print(\"=\"*80)\n",
        "results_voting = evaluate_model(\n",
        "    voting_clf, X_train_tfidf, X_test_tfidf, y_train, y_test\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Crear Stacking Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"CREANDO STACKING CLASSIFIER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Crear Stacking Classifier\n",
        "stacking_clf = create_stacking_classifier(\n",
        "    models_config,\n",
        "    X_train_tfidf,\n",
        "    y_train,\n",
        "    final_estimator=None,  # Usa LogisticRegression por defecto\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Evaluar\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUACI√ìN STACKING CLASSIFIER\")\n",
        "print(\"=\"*80)\n",
        "results_stacking = evaluate_model(\n",
        "    stacking_clf, X_train_tfidf, X_test_tfidf, y_train, y_test\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comparar Ensemble vs Modelos Individuales\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"COMPARACI√ìN: ENSEMBLE VS MODELOS INDIVIDUALES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Comparar Voting vs individuales\n",
        "comparison_voting = compare_ensemble_vs_individual(\n",
        "    voting_clf,\n",
        "    individual_models,\n",
        "    X_train_tfidf,\n",
        "    X_test_tfidf,\n",
        "    y_train,\n",
        "    y_test\n",
        ")\n",
        "\n",
        "print(\"\\nüìä Comparaci√≥n Voting Classifier vs Individuales:\")\n",
        "print(comparison_voting.to_string(index=False))\n",
        "\n",
        "# Comparar Stacking vs individuales\n",
        "comparison_stacking = compare_ensemble_vs_individual(\n",
        "    stacking_clf,\n",
        "    individual_models,\n",
        "    X_train_tfidf,\n",
        "    X_test_tfidf,\n",
        "    y_train,\n",
        "    y_test\n",
        ")\n",
        "\n",
        "print(\"\\nüìä Comparaci√≥n Stacking Classifier vs Individuales:\")\n",
        "print(comparison_stacking.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualizaci√≥n de resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# F1-score comparaci√≥n\n",
        "comparison_all = pd.concat([\n",
        "    comparison_voting.assign(Ensemble_Type='Voting'),\n",
        "    comparison_stacking.assign(Ensemble_Type='Stacking')\n",
        "])\n",
        "\n",
        "# F1-score por modelo\n",
        "pivot_f1 = comparison_all.pivot(index='Modelo', columns='Ensemble_Type', values='F1 (test)')\n",
        "pivot_f1.plot(kind='bar', ax=axes[0, 0], color=['#3498db', '#e74c3c'])\n",
        "axes[0, 0].set_title('F1-Score (Test) - Ensemble vs Individuales', fontweight='bold', fontsize=12)\n",
        "axes[0, 0].set_ylabel('F1-Score')\n",
        "axes[0, 0].set_xlabel('Modelo')\n",
        "axes[0, 0].legend(title='Tipo')\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Overfitting comparaci√≥n\n",
        "pivot_overfitting = comparison_all.pivot(index='Modelo', columns='Ensemble_Type', values='Overfitting (%)')\n",
        "pivot_overfitting.plot(kind='bar', ax=axes[0, 1], color=['#3498db', '#e74c3c'])\n",
        "axes[0, 1].axhline(y=5, color='r', linestyle='--', label='Objetivo (<5%)')\n",
        "axes[0, 1].set_title('Overfitting - Ensemble vs Individuales', fontweight='bold', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Overfitting (%)')\n",
        "axes[0, 1].set_xlabel('Modelo')\n",
        "axes[0, 1].legend(title='Tipo')\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Accuracy comparaci√≥n\n",
        "pivot_acc = comparison_all.pivot(index='Modelo', columns='Ensemble_Type', values='Accuracy (test)')\n",
        "pivot_acc.plot(kind='bar', ax=axes[1, 0], color=['#3498db', '#e74c3c'])\n",
        "axes[1, 0].set_title('Accuracy (Test) - Ensemble vs Individuales', fontweight='bold', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Accuracy')\n",
        "axes[1, 0].set_xlabel('Modelo')\n",
        "axes[1, 0].legend(title='Tipo')\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Scatter: F1 vs Overfitting\n",
        "axes[1, 1].scatter(\n",
        "    comparison_all[comparison_all['Ensemble_Type'] == 'Voting']['Overfitting (%)'],\n",
        "    comparison_all[comparison_all['Ensemble_Type'] == 'Voting']['F1 (test)'],\n",
        "    label='Voting', s=100, alpha=0.7, color='#3498db', marker='o'\n",
        ")\n",
        "axes[1, 1].scatter(\n",
        "    comparison_all[comparison_all['Ensemble_Type'] == 'Stacking']['Overfitting (%)'],\n",
        "    comparison_all[comparison_all['Ensemble_Type'] == 'Stacking']['F1 (test)'],\n",
        "    label='Stacking', s=100, alpha=0.7, color='#e74c3c', marker='s'\n",
        ")\n",
        "axes[1, 1].axvline(x=5, color='r', linestyle='--', alpha=0.5, label='Objetivo Overfitting')\n",
        "axes[1, 1].set_xlabel('Overfitting (%)')\n",
        "axes[1, 1].set_ylabel('F1-Score (Test)')\n",
        "axes[1, 1].set_title('F1-Score vs Overfitting', fontweight='bold', fontsize=12)\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparar todos los modelos (ensembles + individuales)\n",
        "all_results = {\n",
        "    'Voting Classifier': results_voting,\n",
        "    'Stacking Classifier': results_stacking\n",
        "}\n",
        "\n",
        "# A√±adir modelos individuales\n",
        "for name, model in individual_models.items():\n",
        "    all_results[name] = evaluate_model(\n",
        "        model, X_train_tfidf, X_test_tfidf, y_train, y_test, verbose=False\n",
        "    )\n",
        "\n",
        "# Crear comparaci√≥n completa\n",
        "from models.evaluate import compare_models\n",
        "comparison_all = compare_models(all_results)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPARACI√ìN COMPLETA: TODOS LOS MODELOS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n\" + comparison_all.to_string(index=False))\n",
        "\n",
        "# Seleccionar mejor modelo\n",
        "good_models = comparison_all[comparison_all['Overfitting (%)'] < 5.0]\n",
        "\n",
        "if len(good_models) > 0:\n",
        "    best_model_name = good_models.iloc[0]['Modelo']\n",
        "    best_model_results = all_results[best_model_name]\n",
        "    print(f\"\\nüèÜ MEJOR MODELO (Overfitting < 5%): {best_model_name}\")\n",
        "    print(f\"   F1-score (test): {best_model_results['test_f1']:.4f}\")\n",
        "    print(f\"   Overfitting: {best_model_results['diff_f1']:.2f}%\")\n",
        "    \n",
        "    # Determinar qu√© modelo usar\n",
        "    if 'Voting' in best_model_name:\n",
        "        best_ensemble_model = voting_clf\n",
        "        ensemble_type = 'voting'\n",
        "    elif 'Stacking' in best_model_name:\n",
        "        best_ensemble_model = stacking_clf\n",
        "        ensemble_type = 'stacking'\n",
        "    else:\n",
        "        best_ensemble_model = individual_models.get(best_model_name)\n",
        "        ensemble_type = 'individual'\n",
        "else:\n",
        "    # Si ning√∫n modelo cumple, elegir el mejor F1\n",
        "    best_model_name = comparison_all.iloc[0]['Modelo']\n",
        "    best_model_results = all_results[best_model_name]\n",
        "    print(f\"\\n‚ö†Ô∏è  MEJOR MODELO (Overfitting > 5%): {best_model_name}\")\n",
        "    print(f\"   F1-score (test): {best_model_results['test_f1']:.4f}\")\n",
        "    print(f\"   Overfitting: {best_model_results['diff_f1']:.2f}%\")\n",
        "    \n",
        "    if 'Voting' in best_model_name:\n",
        "        best_ensemble_model = voting_clf\n",
        "        ensemble_type = 'voting'\n",
        "    elif 'Stacking' in best_model_name:\n",
        "        best_ensemble_model = stacking_clf\n",
        "        ensemble_type = 'stacking'\n",
        "    else:\n",
        "        best_ensemble_model = individual_models.get(best_model_name)\n",
        "        ensemble_type = 'individual'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar mejor ensemble si es mejor que el modelo individual\n",
        "if 'Voting' in best_model_name or 'Stacking' in best_model_name:\n",
        "    models_dir = Path('../models/ensemble')\n",
        "    models_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    model_path = models_dir / 'best_ensemble_model.pkl'\n",
        "    model_info = {\n",
        "        'model_name': best_model_name,\n",
        "        'ensemble_type': ensemble_type,\n",
        "        'vectorizer_type': 'tfidf',\n",
        "        'test_f1': best_model_results['test_f1'],\n",
        "        'test_accuracy': best_model_results['test_accuracy'],\n",
        "        'overfitting': best_model_results['diff_f1'],\n",
        "        'train_f1': best_model_results['train_f1'],\n",
        "        'models_included': [config['name'] for config in models_config]\n",
        "    }\n",
        "    \n",
        "    save_model(best_ensemble_model, model_path, model_info)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Ensemble guardado:\")\n",
        "    print(f\"   {model_path}\")\n",
        "    print(f\"   Informaci√≥n: {models_dir / 'best_ensemble_model_info.pkl'}\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  El mejor modelo es individual, no se guarda ensemble\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Resumen del ensemble\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"RESUMEN DEL ENSEMBLE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n‚úÖ Ensembles implementados:\")\n",
        "print(f\"   1. Voting Classifier (Soft Voting)\")\n",
        "print(f\"   2. Stacking Classifier\")\n",
        "\n",
        "print(f\"\\n‚úÖ Modelos incluidos en el ensemble:\")\n",
        "for config in models_config:\n",
        "    print(f\"   - {config['name']} ({config['type']})\")\n",
        "\n",
        "print(f\"\\nüèÜ Mejor modelo:\")\n",
        "print(f\"   {best_model_name}\")\n",
        "print(f\"   F1-score (test): {best_model_results['test_f1']:.4f}\")\n",
        "print(f\"   Overfitting: {best_model_results['diff_f1']:.2f}%\")\n",
        "\n",
        "# Verificar si ensemble mejora vs mejor modelo individual\n",
        "best_individual = comparison_all[~comparison_all['Modelo'].str.contains('Classifier')].iloc[0]\n",
        "improvement = best_model_results['test_f1'] - best_individual['F1 (test)']\n",
        "\n",
        "if improvement > 0:\n",
        "    print(f\"\\nüìà Mejora del ensemble vs mejor individual: +{improvement:.4f} F1-score\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Ensemble no mejora vs mejor modelo individual\")\n",
        "\n",
        "print(\"\\n‚úÖ Ensemble completado\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
