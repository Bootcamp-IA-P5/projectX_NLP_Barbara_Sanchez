{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Datos\n",
    "## Dataset: YouToxic English 1000\n",
    "\n",
    "Este notebook contiene todas las técnicas de preprocesamiento aplicadas al texto de los comentarios antes de entrenar los modelos de Machine Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de librerías\n",
    "\n",
    "Importamos todas las librerías necesarias para el preprocesamiento de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Librerías para NLP\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# Librerías para manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Librerías para NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "\n",
    "# Descargar recursos de NLTK (solo la primera vez)\n",
    "print(\"Descargando recursos de NLTK...\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    print(\"✅ punkt_tab descargado\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    print(\"✅ stopwords descargado\")\n",
    "\n",
    "print(\"✅ Recursos de NLTK listos\")\n",
    "\n",
    "# Cargar modelo de spaCy para lematización (inglés)\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    print(\"✅ Modelo de spaCy cargado\")\n",
    "except OSError:\n",
    "    print(\"⚠️  Modelo de spaCy no encontrado. Ejecuta: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Configuración\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga de datos\n",
    "\n",
    "Cargamos el dataset original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv('../data/raw/youtoxic_english_1000.csv')\n",
    "\n",
    "print(f\"Dataset cargado: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verificación de datos\n",
    "\n",
    "Verificamos valores nulos y duplicados antes de empezar el preprocesamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores nulos\n",
    "print(\"Valores nulos por columna:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal de valores nulos: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Verificar duplicados\n",
    "print(f\"\\nNúmero de filas duplicadas: {df.duplicated().sum()}\")\n",
    "print(f\"Número de comentarios duplicados (por texto): {df['Text'].duplicated().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Funciones de preprocesamiento\n",
    "\n",
    "Definimos todas las funciones necesarias para limpiar y preprocesar el texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Limpia el texto eliminando URLs, emails y caracteres especiales.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto a limpiar\n",
    "        \n",
    "    Returns:\n",
    "        str: Texto limpio\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convertir a string si no lo es\n",
    "    text = str(text)\n",
    "    \n",
    "    # Eliminar URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Eliminar emails\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Eliminar caracteres especiales pero mantener espacios y puntuación básica\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Eliminar espacios múltiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "ejemplo = \"Check out this website: https://example.com and email me at test@email.com!!!\"\n",
    "print(\"Texto original:\", ejemplo)\n",
    "print(\"Texto limpio:\", clean_text(ejemplo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normaliza el texto usando expresiones regulares.\n",
    "    - Normaliza contracciones (don't -> do not)\n",
    "    - Normaliza repeticiones de caracteres (sooo -> so)\n",
    "    - Normaliza espacios\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto a normalizar\n",
    "        \n",
    "    Returns:\n",
    "        str: Texto normalizado\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Normalizar contracciones comunes\n",
    "    contractions = {\n",
    "        \"don't\": \"do not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'m\": \" am\",\n",
    "        \"'d\": \" would\"\n",
    "    }\n",
    "    \n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = re.sub(contraction, expansion, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Normalizar repeticiones de caracteres (máximo 2 repeticiones)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    \n",
    "    # Eliminar espacios múltiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "ejemplo = \"I don't think sooo this is gooood!!!\"\n",
    "print(\"Texto original:\", ejemplo)\n",
    "print(\"Texto normalizado:\", normalize_text(ejemplo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokeniza el texto en palabras individuales.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto a tokenizar\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de tokens (palabras)\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return []\n",
    "    \n",
    "    # Usar NLTK para tokenización\n",
    "    tokens = word_tokenize(str(text))\n",
    "    \n",
    "    # Filtrar tokens vacíos\n",
    "    tokens = [token for token in tokens if token.strip()]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "ejemplo = \"This is a sample text for tokenization!\"\n",
    "print(\"Texto original:\", ejemplo)\n",
    "print(\"Tokens:\", tokenize_text(ejemplo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens, language='english'):\n",
    "    \"\"\"\n",
    "    Elimina las stopwords (palabras comunes sin significado) de una lista de tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): Lista de tokens\n",
    "        language (str): Idioma de las stopwords ('english' por defecto)\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de tokens sin stopwords\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    # Obtener stopwords\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    \n",
    "    # Filtrar stopwords\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "ejemplo_tokens = tokenize_text(\"This is a sample text for removing stopwords\")\n",
    "print(\"Tokens originales:\", ejemplo_tokens)\n",
    "print(\"Tokens sin stopwords:\", remove_stopwords(ejemplo_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(tokens):\n",
    "    \"\"\"\n",
    "    Aplica stemming a los tokens (reduce palabras a su raíz).\n",
    "    Usa Porter Stemmer de NLTK.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): Lista de tokens\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de tokens con stemming aplicado\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return stemmed_tokens\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "ejemplo_tokens = tokenize_text(\"running runs ran\")\n",
    "print(\"Tokens originales:\", ejemplo_tokens)\n",
    "print(\"Tokens con stemming:\", stem_text(ejemplo_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text, nlp_model=None):\n",
    "    \"\"\"\n",
    "    Aplica lematización al texto (reduce palabras a su forma base/lema).\n",
    "    Usa spaCy para lematización (más preciso que stemming).\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto a lematizar\n",
    "        nlp_model: Modelo de spaCy cargado\n",
    "        \n",
    "    Returns:\n",
    "        str: Texto lematizado\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    if nlp_model is None:\n",
    "        return str(text)\n",
    "    \n",
    "    # Procesar texto con spaCy\n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    # Extraer lemas\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
    "    \n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "\n",
    "# Ejemplo de uso (si spaCy está disponible)\n",
    "if nlp is not None:\n",
    "    ejemplo = \"running runs ran better\"\n",
    "    print(\"Texto original:\", ejemplo)\n",
    "    print(\"Texto lematizado:\", lemmatize_text(ejemplo, nlp))\n",
    "else:\n",
    "    print(\"⚠️  spaCy no está disponible. Instala el modelo con: python -m spacy download en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pipeline completo de preprocesamiento\n",
    "\n",
    "Aplicamos todas las funciones de preprocesamiento en secuencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, use_lemmatization=True, use_stemming=False):\n",
    "    \"\"\"\n",
    "    Pipeline completo de preprocesamiento de texto.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto original\n",
    "        use_lemmatization (bool): Si True, usa lematización (requiere spaCy)\n",
    "        use_stemming (bool): Si True, usa stemming (solo si no se usa lematización)\n",
    "        \n",
    "    Returns:\n",
    "        str: Texto preprocesado\n",
    "    \"\"\"\n",
    "    # 1. Limpieza básica\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # 2. Normalización\n",
    "    text = normalize_text(text)\n",
    "    \n",
    "    # 3. Tokenización\n",
    "    tokens = tokenize_text(text)\n",
    "    \n",
    "    # 4. Eliminación de stopwords\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    # 5. Stemming o Lematización\n",
    "    if use_lemmatization and nlp is not None:\n",
    "        # Para lematización, necesitamos el texto completo\n",
    "        text_for_lemma = ' '.join(tokens)\n",
    "        processed_text = lemmatize_text(text_for_lemma, nlp)\n",
    "    elif use_stemming:\n",
    "        # Aplicar stemming\n",
    "        tokens = stem_text(tokens)\n",
    "        processed_text = ' '.join(tokens)\n",
    "    else:\n",
    "        # Sin stemming ni lematización\n",
    "        processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text.strip()\n",
    "\n",
    "\n",
    "# Ejemplo completo\n",
    "ejemplo = \"Check out https://example.com! I don't think sooo this is gooood!!!\"\n",
    "print(\"Texto original:\")\n",
    "print(ejemplo)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Texto preprocesado (con lematización):\")\n",
    "print(preprocess_text(ejemplo, use_lemmatization=True, use_stemming=False))\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Texto preprocesado (con stemming):\")\n",
    "print(preprocess_text(ejemplo, use_lemmatization=False, use_stemming=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Aplicar preprocesamiento al dataset\n",
    "\n",
    "Aplicamos el preprocesamiento a todos los comentarios del dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una copia del dataframe para trabajar\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Aplicar preprocesamiento con lematización (más preciso)\n",
    "print(\"Aplicando preprocesamiento con lematización...\")\n",
    "print(\"Esto puede tardar unos minutos...\")\n",
    "\n",
    "if nlp is not None:\n",
    "    df_processed['Text_processed'] = df_processed['Text'].apply(\n",
    "        lambda x: preprocess_text(x, use_lemmatization=True, use_stemming=False)\n",
    "    )\n",
    "else:\n",
    "    print(\"⚠️  Usando stemming porque spaCy no está disponible\")\n",
    "    df_processed['Text_processed'] = df_processed['Text'].apply(\n",
    "        lambda x: preprocess_text(x, use_lemmatization=False, use_stemming=True)\n",
    "    )\n",
    "\n",
    "print(\"✅ Preprocesamiento completado!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver ejemplos de texto original vs preprocesado\n",
    "print(\"=\"*80)\n",
    "print(\"EJEMPLOS: Texto Original vs Preprocesado\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nEjemplo {i+1}:\")\n",
    "    print(f\"Original: {df_processed.iloc[i]['Text'][:150]}...\")\n",
    "    print(f\"Preprocesado: {df_processed.iloc[i]['Text_processed'][:150]}...\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Manejo de valores nulos y duplicados\n",
    "\n",
    "Verificamos y manejamos cualquier problema restante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay textos preprocesados vacíos\n",
    "empty_processed = df_processed['Text_processed'].str.strip() == ''\n",
    "print(f\"Textos preprocesados vacíos: {empty_processed.sum()}\")\n",
    "\n",
    "# Mostrar algunos ejemplos si los hay\n",
    "if empty_processed.sum() > 0:\n",
    "    print(\"\\nEjemplos de textos que quedaron vacíos después del preprocesamiento:\")\n",
    "    empty_examples = df_processed[empty_processed][['Text', 'Text_processed']].head(5)\n",
    "    print(empty_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con texto preprocesado vacío (si las hay)\n",
    "if empty_processed.sum() > 0:\n",
    "    print(f\"Eliminando {empty_processed.sum()} filas con texto vacío...\")\n",
    "    df_processed = df_processed[~empty_processed].copy()\n",
    "    print(f\"Dataset después de eliminar vacíos: {df_processed.shape[0]} filas\")\n",
    "\n",
    "# Verificar duplicados en el texto preprocesado\n",
    "duplicated_processed = df_processed['Text_processed'].duplicated()\n",
    "print(f\"\\nTextos preprocesados duplicados: {duplicated_processed.sum()}\")\n",
    "\n",
    "# Decisión: mantener duplicados por ahora (pueden ser comentarios legítimamente iguales)\n",
    "# Si queremos eliminarlos, descomentar la siguiente línea:\n",
    "# df_processed = df_processed[~duplicated_processed].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Guardar datos preprocesados\n",
    "\n",
    "Guardamos el dataset preprocesado para usarlo en el modelado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio processed si no existe\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Guardar dataset preprocesado\n",
    "output_path = '../data/processed/youtoxic_english_1000_processed.csv'\n",
    "df_processed.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Dataset preprocesado guardado en: {output_path}\")\n",
    "print(f\"Shape del dataset guardado: {df_processed.shape}\")\n",
    "print(f\"\\nColumnas guardadas:\")\n",
    "print(df_processed.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resumen del preprocesamiento\n",
    "\n",
    "Resumen de las transformaciones aplicadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RESUMEN DEL PREPROCESAMIENTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. DATASET:\")\n",
    "print(f\"   - Filas originales: {len(df)}\")\n",
    "print(f\"   - Filas después del preprocesamiento: {len(df_processed)}\")\n",
    "print(f\"   - Filas eliminadas: {len(df) - len(df_processed)}\")\n",
    "\n",
    "print(f\"\\n2. TRANSFORMACIONES APLICADAS:\")\n",
    "print(f\"   ✅ Limpieza de URLs y emails\")\n",
    "print(f\"   ✅ Eliminación de caracteres especiales\")\n",
    "print(f\"   ✅ Normalización de texto (contracciones, repeticiones)\")\n",
    "print(f\"   ✅ Tokenización\")\n",
    "print(f\"   ✅ Eliminación de stopwords\")\n",
    "if nlp is not None:\n",
    "    print(f\"   ✅ Lematización (usando spaCy)\")\n",
    "else:\n",
    "    print(f\"   ✅ Stemming (usando NLTK)\")\n",
    "\n",
    "print(f\"\\n3. ESTADÍSTICAS DEL TEXTO PREPROCESADO:\")\n",
    "df_processed['text_length_original'] = df_processed['Text'].str.len()\n",
    "df_processed['text_length_processed'] = df_processed['Text_processed'].str.len()\n",
    "df_processed['word_count_processed'] = df_processed['Text_processed'].str.split().str.len()\n",
    "\n",
    "print(f\"   - Longitud promedio original: {df_processed['text_length_original'].mean():.1f} caracteres\")\n",
    "print(f\"   - Longitud promedio preprocesado: {df_processed['text_length_processed'].mean():.1f} caracteres\")\n",
    "print(f\"   - Palabras promedio preprocesado: {df_processed['word_count_processed'].mean():.1f} palabras\")\n",
    "\n",
    "print(f\"\\n4. ARCHIVO GUARDADO:\")\n",
    "print(f\"   - Ruta: ../data/processed/youtoxic_english_1000_processed.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
