{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Datos\n",
    "## Dataset: YouToxic English 1000\n",
    "\n",
    "Este notebook contiene todas las técnicas de preprocesamiento aplicadas al texto de los comentarios antes de entrenar los modelos de Machine Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de librerías\n",
    "\n",
    "Importamos todas las librerías necesarias para el preprocesamiento de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando recursos de NLTK...\n",
      "✅ Recursos de NLTK listos\n",
      "✅ Modelo de spaCy cargado\n"
     ]
    }
   ],
   "source": [
    "# Librerías para manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Librerías para NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "\n",
    "# Descargar recursos de NLTK (solo la primera vez)\n",
    "print(\"Descargando recursos de NLTK...\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    print(\"✅ punkt_tab descargado\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    print(\"✅ stopwords descargado\")\n",
    "\n",
    "print(\"✅ Recursos de NLTK listos\")\n",
    "\n",
    "# Cargar modelo de spaCy para lematización (inglés)\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    print(\"✅ Modelo de spaCy cargado\")\n",
    "except OSError:\n",
    "    print(\"⚠️  Modelo de spaCy no encontrado. Ejecuta: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Configuración\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga de datos\n",
    "\n",
    "Cargamos el dataset original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado: 1000 filas, 15 columnas\n",
      "\n",
      "Primeras filas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommentId</th>\n",
       "      <th>VideoId</th>\n",
       "      <th>Text</th>\n",
       "      <th>IsToxic</th>\n",
       "      <th>IsAbusive</th>\n",
       "      <th>IsThreat</th>\n",
       "      <th>IsProvocative</th>\n",
       "      <th>IsObscene</th>\n",
       "      <th>IsHatespeech</th>\n",
       "      <th>IsRacist</th>\n",
       "      <th>IsNationalist</th>\n",
       "      <th>IsSexist</th>\n",
       "      <th>IsHomophobic</th>\n",
       "      <th>IsReligiousHate</th>\n",
       "      <th>IsRadicalism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ugg2KwwX0V8-aXgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>If only people would just take a step back and not make this case about them, because it wasn't about anyone except the two people in that situati...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ugg2s5AzSPioEXgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>Law enforcement is not trained to shoot to apprehend.  They are trained to shoot to kill.  And I thank Wilson for killing that punk bitch.</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ugg3dWTOxryFfHgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>\\nDont you reckon them 'black lives matter' banners being held by white cunts is  kinda patronizing and ironically racist. could they have not com...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ugg7Gd006w1MPngCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>There are a very large number of people who do not like police officers. They are called Criminals and its the reason we have police officers. The...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ugg8FfTbbNF8IngCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>The Arab dude is absolutely right, he should have not been shot 6 extra time. Shoot him once if hes attacking you and that would stop his attack. ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              CommentId      VideoId  \\\n",
       "0  Ugg2KwwX0V8-aXgCoAEC  04kJtp6pVXI   \n",
       "1  Ugg2s5AzSPioEXgCoAEC  04kJtp6pVXI   \n",
       "2  Ugg3dWTOxryFfHgCoAEC  04kJtp6pVXI   \n",
       "3  Ugg7Gd006w1MPngCoAEC  04kJtp6pVXI   \n",
       "4  Ugg8FfTbbNF8IngCoAEC  04kJtp6pVXI   \n",
       "\n",
       "                                                                                                                                                    Text  \\\n",
       "0  If only people would just take a step back and not make this case about them, because it wasn't about anyone except the two people in that situati...   \n",
       "1             Law enforcement is not trained to shoot to apprehend.  They are trained to shoot to kill.  And I thank Wilson for killing that punk bitch.   \n",
       "2  \\nDont you reckon them 'black lives matter' banners being held by white cunts is  kinda patronizing and ironically racist. could they have not com...   \n",
       "3  There are a very large number of people who do not like police officers. They are called Criminals and its the reason we have police officers. The...   \n",
       "4  The Arab dude is absolutely right, he should have not been shot 6 extra time. Shoot him once if hes attacking you and that would stop his attack. ...   \n",
       "\n",
       "   IsToxic  IsAbusive  IsThreat  IsProvocative  IsObscene  IsHatespeech  \\\n",
       "0    False      False     False          False      False         False   \n",
       "1     True       True     False          False      False         False   \n",
       "2     True       True     False          False       True         False   \n",
       "3    False      False     False          False      False         False   \n",
       "4    False      False     False          False      False         False   \n",
       "\n",
       "   IsRacist  IsNationalist  IsSexist  IsHomophobic  IsReligiousHate  \\\n",
       "0     False          False     False         False            False   \n",
       "1     False          False     False         False            False   \n",
       "2     False          False     False         False            False   \n",
       "3     False          False     False         False            False   \n",
       "4     False          False     False         False            False   \n",
       "\n",
       "   IsRadicalism  \n",
       "0         False  \n",
       "1         False  \n",
       "2         False  \n",
       "3         False  \n",
       "4         False  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv('../data/raw/youtoxic_english_1000.csv')\n",
    "\n",
    "print(f\"Dataset cargado: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verificación de datos\n",
    "\n",
    "Verificamos valores nulos y duplicados antes de empezar el preprocesamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos por columna:\n",
      "CommentId          0\n",
      "VideoId            0\n",
      "Text               0\n",
      "IsToxic            0\n",
      "IsAbusive          0\n",
      "IsThreat           0\n",
      "IsProvocative      0\n",
      "IsObscene          0\n",
      "IsHatespeech       0\n",
      "IsRacist           0\n",
      "IsNationalist      0\n",
      "IsSexist           0\n",
      "IsHomophobic       0\n",
      "IsReligiousHate    0\n",
      "IsRadicalism       0\n",
      "dtype: int64\n",
      "\n",
      "Total de valores nulos: 0\n",
      "\n",
      "Número de filas duplicadas: 0\n",
      "Número de comentarios duplicados (por texto): 3\n"
     ]
    }
   ],
   "source": [
    "# Verificar valores nulos\n",
    "print(\"Valores nulos por columna:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal de valores nulos: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Verificar duplicados\n",
    "print(f\"\\nNúmero de filas duplicadas: {df.duplicated().sum()}\")\n",
    "print(f\"Número de comentarios duplicados (por texto): {df['Text'].duplicated().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Funciones de preprocesamiento\n",
    "\n",
    "Definimos todas las funciones necesarias para limpiar y preprocesar el texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: Check out this website: https://example.com and email me at test@email.com!!!\n",
      "Texto limpio: check out this website and email me at\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Limpia el texto eliminando URLs, emails y caracteres especiales.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto a limpiar\n",
    "        \n",
    "    Returns:\n",
    "        str: Texto limpio\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convertir a string si no lo es\n",
    "    text = str(text)\n",
    "    \n",
    "    # Eliminar URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Eliminar emails\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Eliminar caracteres especiales pero mantener espacios y puntuación básica\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Eliminar espacios múltiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "ejemplo = \"Check out this website: https://example.com and email me at test@email.com!!!\"\n",
    "print(\"Texto original:\", ejemplo)\n",
    "print(\"Texto limpio:\", clean_text(ejemplo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: I don't think sooo this is gooood!!!\n",
      "Texto normalizado: I do not think soo this is good!!\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normaliza el texto usando expresiones regulares.\n",
    "    - Normaliza contracciones (don't -> do not)\n",
    "    - Normaliza repeticiones de caracteres (sooo -> so)\n",
    "    - Normaliza espacios\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto a normalizar\n",
    "        \n",
    "    Returns:\n",
    "        str: Texto normalizado\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Normalizar contracciones comunes\n",
    "    contractions = {\n",
    "        \"don't\": \"do not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'m\": \" am\",\n",
    "        \"'d\": \" would\"\n",
    "    }\n",
    "    \n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = re.sub(contraction, expansion, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Normalizar repeticiones de caracteres (máximo 2 repeticiones)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    \n",
    "    # Eliminar espacios múltiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "ejemplo = \"I don't think sooo this is gooood!!!\"\n",
    "print(\"Texto original:\", ejemplo)\n",
    "print(\"Texto normalizado:\", normalize_text(ejemplo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: This is a sample text for tokenization!\n",
      "Tokens: ['This', 'is', 'a', 'sample', 'text', 'for', 'tokenization', '!']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokeniza el texto en palabras individuales.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto a tokenizar\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de tokens (palabras)\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return []\n",
    "    \n",
    "    # Usar NLTK para tokenización\n",
    "    tokens = word_tokenize(str(text))\n",
    "    \n",
    "    # Filtrar tokens vacíos\n",
    "    tokens = [token for token in tokens if token.strip()]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "ejemplo = \"This is a sample text for tokenization!\"\n",
    "print(\"Texto original:\", ejemplo)\n",
    "print(\"Tokens:\", tokenize_text(ejemplo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens originales: ['This', 'is', 'a', 'sample', 'text', 'for', 'removing', 'stopwords']\n",
      "Tokens sin stopwords: ['sample', 'text', 'removing', 'stopwords']\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(tokens, language='english'):\n",
    "    \"\"\"\n",
    "    Elimina las stopwords (palabras comunes sin significado) de una lista de tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): Lista de tokens\n",
    "        language (str): Idioma de las stopwords ('english' por defecto)\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de tokens sin stopwords\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    # Obtener stopwords\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    \n",
    "    # Filtrar stopwords\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "ejemplo_tokens = tokenize_text(\"This is a sample text for removing stopwords\")\n",
    "print(\"Tokens originales:\", ejemplo_tokens)\n",
    "print(\"Tokens sin stopwords:\", remove_stopwords(ejemplo_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens originales: ['running', 'runs', 'ran']\n",
      "Tokens con stemming: ['run', 'run', 'ran']\n"
     ]
    }
   ],
   "source": [
    "def stem_text(tokens):\n",
    "    \"\"\"\n",
    "    Aplica stemming a los tokens (reduce palabras a su raíz).\n",
    "    Usa Porter Stemmer de NLTK.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): Lista de tokens\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de tokens con stemming aplicado\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return stemmed_tokens\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "ejemplo_tokens = tokenize_text(\"running runs ran\")\n",
    "print(\"Tokens originales:\", ejemplo_tokens)\n",
    "print(\"Tokens con stemming:\", stem_text(ejemplo_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: running runs ran better\n",
      "Texto lematizado: run run run well\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_text(text, nlp_model=None):\n",
    "    \"\"\"\n",
    "    Aplica lematización al texto (reduce palabras a su forma base/lema).\n",
    "    Usa spaCy para lematización (más preciso que stemming).\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto a lematizar\n",
    "        nlp_model: Modelo de spaCy cargado\n",
    "        \n",
    "    Returns:\n",
    "        str: Texto lematizado\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    if nlp_model is None:\n",
    "        return str(text)\n",
    "    \n",
    "    # Procesar texto con spaCy\n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    # Extraer lemas\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
    "    \n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "\n",
    "# Ejemplo de uso (si spaCy está disponible)\n",
    "if nlp is not None:\n",
    "    ejemplo = \"running runs ran better\"\n",
    "    print(\"Texto original:\", ejemplo)\n",
    "    print(\"Texto lematizado:\", lemmatize_text(ejemplo, nlp))\n",
    "else:\n",
    "    print(\"⚠️  spaCy no está disponible. Instala el modelo con: python -m spacy download en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pipeline completo de preprocesamiento\n",
    "\n",
    "Aplicamos todas las funciones de preprocesamiento en secuencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original:\n",
      "Check out https://example.com! I don't think sooo this is gooood!!!\n",
      "\n",
      "============================================================\n",
      "Texto preprocesado (con lematización):\n",
      "check do not think soo good\n",
      "\n",
      "============================================================\n",
      "Texto preprocesado (con stemming):\n",
      "check dont think soo good\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text, use_lemmatization=True, use_stemming=False):\n",
    "    \"\"\"\n",
    "    Pipeline completo de preprocesamiento de texto.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texto original\n",
    "        use_lemmatization (bool): Si True, usa lematización (requiere spaCy)\n",
    "        use_stemming (bool): Si True, usa stemming (solo si no se usa lematización)\n",
    "        \n",
    "    Returns:\n",
    "        str: Texto preprocesado\n",
    "    \"\"\"\n",
    "    # 1. Limpieza básica\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # 2. Normalización\n",
    "    text = normalize_text(text)\n",
    "    \n",
    "    # 3. Tokenización\n",
    "    tokens = tokenize_text(text)\n",
    "    \n",
    "    # 4. Eliminación de stopwords\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    # 5. Stemming o Lematización\n",
    "    if use_lemmatization and nlp is not None:\n",
    "        # Para lematización, necesitamos el texto completo\n",
    "        text_for_lemma = ' '.join(tokens)\n",
    "        processed_text = lemmatize_text(text_for_lemma, nlp)\n",
    "    elif use_stemming:\n",
    "        # Aplicar stemming\n",
    "        tokens = stem_text(tokens)\n",
    "        processed_text = ' '.join(tokens)\n",
    "    else:\n",
    "        # Sin stemming ni lematización\n",
    "        processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text.strip()\n",
    "\n",
    "\n",
    "# Ejemplo completo\n",
    "ejemplo = \"Check out https://example.com! I don't think sooo this is gooood!!!\"\n",
    "print(\"Texto original:\")\n",
    "print(ejemplo)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Texto preprocesado (con lematización):\")\n",
    "print(preprocess_text(ejemplo, use_lemmatization=True, use_stemming=False))\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Texto preprocesado (con stemming):\")\n",
    "print(preprocess_text(ejemplo, use_lemmatization=False, use_stemming=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Aplicar preprocesamiento al dataset\n",
    "\n",
    "Aplicamos el preprocesamiento a todos los comentarios del dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando preprocesamiento con lematización...\n",
      "Esto puede tardar unos minutos...\n",
      "✅ Preprocesamiento completado!\n"
     ]
    }
   ],
   "source": [
    "# Crear una copia del dataframe para trabajar\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Aplicar preprocesamiento con lematización (más preciso)\n",
    "print(\"Aplicando preprocesamiento con lematización...\")\n",
    "print(\"Esto puede tardar unos minutos...\")\n",
    "\n",
    "if nlp is not None:\n",
    "    df_processed['Text_processed'] = df_processed['Text'].apply(\n",
    "        lambda x: preprocess_text(x, use_lemmatization=True, use_stemming=False)\n",
    "    )\n",
    "else:\n",
    "    print(\"⚠️  Usando stemming porque spaCy no está disponible\")\n",
    "    df_processed['Text_processed'] = df_processed['Text'].apply(\n",
    "        lambda x: preprocess_text(x, use_lemmatization=False, use_stemming=True)\n",
    "    )\n",
    "\n",
    "print(\"✅ Preprocesamiento completado!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EJEMPLOS: Texto Original vs Preprocesado\n",
      "================================================================================\n",
      "\n",
      "Ejemplo 1:\n",
      "Original: If only people would just take a step back and not make this case about them, because it wasn't about anyone except the two people in that situation. ...\n",
      "Preprocesado: people would take step back make case be not anyone except two people situation lump mess take matter hand make kind protest selfish without rational ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Ejemplo 2:\n",
      "Original: Law enforcement is not trained to shoot to apprehend.  They are trained to shoot to kill.  And I thank Wilson for killing that punk bitch....\n",
      "Preprocesado: law enforcement train shoot apprehend train shoot kill thank wilson kill punk bitch...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Ejemplo 3:\n",
      "Original: \n",
      "Dont you reckon them 'black lives matter' banners being held by white cunts is  kinda patronizing and ironically racist. could they have not come up ...\n",
      "Preprocesado: do not reckon black life matter banner hold white cunt kinda patronize ironically racist could come somethin well white folk give self pride oo look I...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Ejemplo 4:\n",
      "Original: There are a very large number of people who do not like police officers. They are called Criminals and its the reason we have police officers. The fac...\n",
      "Preprocesado: large number people like police officer call criminal reason police officer fact criminal like police officer testament good work police officer prote...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Ejemplo 5:\n",
      "Original: The Arab dude is absolutely right, he should have not been shot 6 extra time. Shoot him once if hes attacking you and that would stop his attack. Shoo...\n",
      "Preprocesado: arab dude absolutely right shot extra time shoot he s attack would stop attack shoot twice he s still attack six time shoot kill opinion...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ver ejemplos de texto original vs preprocesado\n",
    "print(\"=\"*80)\n",
    "print(\"EJEMPLOS: Texto Original vs Preprocesado\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nEjemplo {i+1}:\")\n",
    "    print(f\"Original: {df_processed.iloc[i]['Text'][:150]}...\")\n",
    "    print(f\"Preprocesado: {df_processed.iloc[i]['Text_processed'][:150]}...\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Manejo de valores nulos y duplicados\n",
    "\n",
    "Verificamos y manejamos cualquier problema restante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textos preprocesados vacíos: 0\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay textos preprocesados vacíos\n",
    "empty_processed = df_processed['Text_processed'].str.strip() == ''\n",
    "print(f\"Textos preprocesados vacíos: {empty_processed.sum()}\")\n",
    "\n",
    "# Mostrar algunos ejemplos si los hay\n",
    "if empty_processed.sum() > 0:\n",
    "    print(\"\\nEjemplos de textos que quedaron vacíos después del preprocesamiento:\")\n",
    "    empty_examples = df_processed[empty_processed][['Text', 'Text_processed']].head(5)\n",
    "    print(empty_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Textos preprocesados duplicados: 8\n"
     ]
    }
   ],
   "source": [
    "# Eliminar filas con texto preprocesado vacío (si las hay)\n",
    "if empty_processed.sum() > 0:\n",
    "    print(f\"Eliminando {empty_processed.sum()} filas con texto vacío...\")\n",
    "    df_processed = df_processed[~empty_processed].copy()\n",
    "    print(f\"Dataset después de eliminar vacíos: {df_processed.shape[0]} filas\")\n",
    "\n",
    "# Verificar duplicados en el texto preprocesado\n",
    "duplicated_processed = df_processed['Text_processed'].duplicated()\n",
    "print(f\"\\nTextos preprocesados duplicados: {duplicated_processed.sum()}\")\n",
    "\n",
    "# Decisión: mantener duplicados por ahora (pueden ser comentarios legítimamente iguales)\n",
    "# Si queremos eliminarlos, descomentar la siguiente línea:\n",
    "# df_processed = df_processed[~duplicated_processed].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Guardar datos preprocesados\n",
    "\n",
    "Guardamos el dataset preprocesado para usarlo en el modelado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset preprocesado guardado en: ../data/processed/youtoxic_english_1000_processed.csv\n",
      "Shape del dataset guardado: (1000, 16)\n",
      "\n",
      "Columnas guardadas:\n",
      "['CommentId', 'VideoId', 'Text', 'IsToxic', 'IsAbusive', 'IsThreat', 'IsProvocative', 'IsObscene', 'IsHatespeech', 'IsRacist', 'IsNationalist', 'IsSexist', 'IsHomophobic', 'IsReligiousHate', 'IsRadicalism', 'Text_processed']\n"
     ]
    }
   ],
   "source": [
    "# Crear directorio processed si no existe\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Guardar dataset preprocesado\n",
    "output_path = '../data/processed/youtoxic_english_1000_processed.csv'\n",
    "df_processed.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Dataset preprocesado guardado en: {output_path}\")\n",
    "print(f\"Shape del dataset guardado: {df_processed.shape}\")\n",
    "print(f\"\\nColumnas guardadas:\")\n",
    "print(df_processed.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resumen del preprocesamiento\n",
    "\n",
    "Resumen de las transformaciones aplicadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RESUMEN DEL PREPROCESAMIENTO\n",
      "================================================================================\n",
      "\n",
      "1. DATASET:\n",
      "   - Filas originales: 1000\n",
      "   - Filas después del preprocesamiento: 1000\n",
      "   - Filas eliminadas: 0\n",
      "\n",
      "2. TRANSFORMACIONES APLICADAS:\n",
      "   ✅ Limpieza de URLs y emails\n",
      "   ✅ Eliminación de caracteres especiales\n",
      "   ✅ Normalización de texto (contracciones, repeticiones)\n",
      "   ✅ Tokenización\n",
      "   ✅ Eliminación de stopwords\n",
      "   ✅ Lematización (usando spaCy)\n",
      "\n",
      "3. ESTADÍSTICAS DEL TEXTO PREPROCESADO:\n",
      "   - Longitud promedio original: 185.6 caracteres\n",
      "   - Longitud promedio preprocesado: 109.7 caracteres\n",
      "   - Palabras promedio preprocesado: 17.8 palabras\n",
      "\n",
      "4. ARCHIVO GUARDADO:\n",
      "   - Ruta: ../data/processed/youtoxic_english_1000_processed.csv\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RESUMEN DEL PREPROCESAMIENTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. DATASET:\")\n",
    "print(f\"   - Filas originales: {len(df)}\")\n",
    "print(f\"   - Filas después del preprocesamiento: {len(df_processed)}\")\n",
    "print(f\"   - Filas eliminadas: {len(df) - len(df_processed)}\")\n",
    "\n",
    "print(f\"\\n2. TRANSFORMACIONES APLICADAS:\")\n",
    "print(f\"   ✅ Limpieza de URLs y emails\")\n",
    "print(f\"   ✅ Eliminación de caracteres especiales\")\n",
    "print(f\"   ✅ Normalización de texto (contracciones, repeticiones)\")\n",
    "print(f\"   ✅ Tokenización\")\n",
    "print(f\"   ✅ Eliminación de stopwords\")\n",
    "if nlp is not None:\n",
    "    print(f\"   ✅ Lematización (usando spaCy)\")\n",
    "else:\n",
    "    print(f\"   ✅ Stemming (usando NLTK)\")\n",
    "\n",
    "print(f\"\\n3. ESTADÍSTICAS DEL TEXTO PREPROCESADO:\")\n",
    "df_processed['text_length_original'] = df_processed['Text'].str.len()\n",
    "df_processed['text_length_processed'] = df_processed['Text_processed'].str.len()\n",
    "df_processed['word_count_processed'] = df_processed['Text_processed'].str.split().str.len()\n",
    "\n",
    "print(f\"   - Longitud promedio original: {df_processed['text_length_original'].mean():.1f} caracteres\")\n",
    "print(f\"   - Longitud promedio preprocesado: {df_processed['text_length_processed'].mean():.1f} caracteres\")\n",
    "print(f\"   - Palabras promedio preprocesado: {df_processed['word_count_processed'].mean():.1f} palabras\")\n",
    "\n",
    "print(f\"\\n4. ARCHIVO GUARDADO:\")\n",
    "print(f\"   - Ruta: ../data/processed/youtoxic_english_1000_processed.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
