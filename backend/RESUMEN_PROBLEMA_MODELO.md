# üìä Resumen: Problema de Probabilidades Similares

## ‚úÖ Lo que S√ç funciona

1. **El modelo predice correctamente**:
   - Textos positivos ‚Üí `is_toxic=False`
   - Textos negativos ‚Üí `is_toxic=True`

2. **Los vectores son diferentes**:
   - Diferentes textos producen diferentes vectores
   - El modelo recibe informaci√≥n diferente

3. **El c√≥digo est√° correcto**:
   - `class_weight='balanced'` est√° configurado
   - `probability=True` est√° configurado
   - El preprocesamiento funciona

## üî¥ El Problema Real

**Las probabilidades est√°n mal calibradas** - todas muy cerca de 0.5:

```
Texto positivo:  probability_toxic = 0.45
Texto negativo:  probability_toxic = 0.47
Diferencia: solo 0.02 (2%)
```

### ¬øPor qu√© pasa esto?

1. **SVM no produce probabilidades naturales**
   - SVM usa `predict_proba` que no est√° bien calibrado
   - Las probabilidades de SVM tienden a estar cerca de 0.5

2. **Vocabulario limitado (1000 palabras)**
   - Muchos textos se vectorizan de forma similar
   - El modelo no puede diferenciar bien

3. **Modelo d√©bil**
   - F1-score: 0.6866 (no es muy alto)
   - Accuracy: 0.58 (muy bajo, casi al azar)
   - El modelo no est√° aprendiendo bien

## üìç D√≥nde Revisar

### 1. **Notebook de Entrenamiento** (PRINCIPAL)
**Archivo**: `backend/notebooks/05_Hyperparameter_Tuning.ipynb`

**Qu√© buscar:**
- M√©tricas de evaluaci√≥n (l√≠nea ~250-260)
- C√≥mo se guarda el modelo (l√≠nea ~340-473)
- Si se us√≥ calibraci√≥n de probabilidades

**Comandos √∫tiles:**
```python
# Ver m√©tricas del modelo guardado
import pickle
info = pickle.load(open('models/optimized/best_optimized_model_info.pkl', 'rb'))
print(info)
```

### 2. **Dataset de Entrenamiento**
**Archivo**: `data/processed/youtoxic_english_1000_processed.csv`

**Verificar:**
- Tama√±o del dataset (1000 ejemplos es muy poco)
- Calidad de las etiquetas
- Balance de clases

### 3. **M√≥dulo de Optimizaci√≥n**
**Archivo**: `backend/src/models/optimization.py`

**L√≠nea 341**: Verificar que se pasan todos los par√°metros al guardar

### 4. **M√≥dulo de Entrenamiento**
**Archivo**: `backend/src/models/train.py`

**L√≠nea 97-103**: Verificar creaci√≥n del modelo SVC

## üîß Soluciones Posibles

### Soluci√≥n 1: Calibrar Probabilidades (Recomendado)
```python
from sklearn.calibration import CalibratedClassifierCV

# Calibrar el modelo
calibrated_model = CalibratedClassifierCV(
    model, 
    cv=5, 
    method='sigmoid'  # o 'isotonic'
)
calibrated_model.fit(X_train, y_train)
```

### Soluci√≥n 2: Usar Modelo Diferente
- **Logistic Regression**: Mejores probabilidades calibradas
- **Random Forest**: Probabilidades m√°s confiables
- **Naive Bayes**: Probabilidades naturales

### Soluci√≥n 3: Aumentar Vocabulario
```python
# En vectorizaci√≥n, aumentar max_features
vectorizer = TfidfVectorizer(max_features=5000)  # En lugar de 1000
```

### Soluci√≥n 4: M√°s Datos de Entrenamiento
- 1000 ejemplos es muy poco
- Necesitas al menos 5000-10000 ejemplos

## üéØ Acci√≥n Inmediata

1. **Abrir**: `backend/notebooks/05_Hyperparameter_Tuning.ipynb`
2. **Buscar**: Celda donde se eval√∫a el modelo final
3. **Verificar**: M√©tricas de test (especialmente Recall)
4. **Revisar**: Si se us√≥ calibraci√≥n de probabilidades
5. **Si no**: A√±adir calibraci√≥n y re-entrenar

## üìù Nota Final

**El problema NO es del c√≥digo**, es del modelo:
- El modelo est√° prediciendo correctamente (clases)
- Pero las probabilidades no est√°n bien calibradas
- Esto es normal en SVM sin calibraci√≥n

**La soluci√≥n es calibrar las probabilidades o usar un modelo diferente.**

