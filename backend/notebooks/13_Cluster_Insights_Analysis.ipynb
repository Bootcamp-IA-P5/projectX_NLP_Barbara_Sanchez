{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîç An√°lisis Profundo de Clusters - Insights y Aplicaciones\n",
        "\n",
        "Este notebook analiza los resultados del clustering para extraer insights pr√°cticos y aplicarlos al proyecto.\n",
        "\n",
        "## Objetivos:\n",
        "1. Analizar caracter√≠sticas de cada cluster (palabras frecuentes, longitud, etc.)\n",
        "2. Identificar subcategor√≠as de hate speech\n",
        "3. Analizar outliers encontrados por DBSCAN\n",
        "4. Generar recomendaciones para mejorar el modelo\n",
        "5. Documentar insights para EDA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importar librer√≠as y cargar datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# A√±adir src al path\n",
        "sys.path.append(str(Path('../src').resolve()))\n",
        "\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Cargar datos originales para an√°lisis de texto\n",
        "data_path = Path('../data/raw/youtoxic_english_1000.csv')\n",
        "df_original = pd.read_csv(data_path)\n",
        "\n",
        "# Cargar datos vectorizados\n",
        "from features.vectorization import load_vectorized_data\n",
        "data_dir = Path('../data/processed')\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = load_vectorized_data(data_dir, prefix='tfidf')\n",
        "\n",
        "# Combinar train y test\n",
        "X_all = np.vstack([X_train_tfidf, X_test_tfidf])\n",
        "y_all = pd.concat([y_train, y_test], ignore_index=True)\n",
        "\n",
        "print(\"‚úÖ Datos cargados:\")\n",
        "print(f\"   Total ejemplos: {len(df_original)}\")\n",
        "print(f\"   T√≥xicos: {y_all.sum()}\")\n",
        "print(f\"   No t√≥xicos: {len(y_all) - y_all.sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Aplicar KMeans k=2 y obtener clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reducir dimensionalidad y aplicar KMeans\n",
        "X_dense = X_all.toarray() if hasattr(X_all, 'toarray') else X_all\n",
        "pca_kmeans = PCA(n_components=100, random_state=42)\n",
        "X_reduced = pca_kmeans.fit_transform(X_dense)\n",
        "\n",
        "# Aplicar KMeans k=2\n",
        "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(X_reduced)\n",
        "\n",
        "# A√±adir clusters al DataFrame original\n",
        "df_analysis = df_original.copy()\n",
        "df_analysis['cluster'] = cluster_labels\n",
        "df_analysis['is_toxic'] = y_all.values\n",
        "\n",
        "print(\"‚úÖ Clusters asignados:\")\n",
        "print(f\"   Cluster 0: {sum(cluster_labels == 0)} ejemplos\")\n",
        "print(f\"   Cluster 1: {sum(cluster_labels == 1)} ejemplos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. An√°lisis de Caracter√≠sticas por Cluster\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_cluster(df, cluster_id, text_column='Text'):\n",
        "    \"\"\"Analizar caracter√≠sticas de un cluster.\"\"\"\n",
        "    cluster_data = df[df['cluster'] == cluster_id]\n",
        "    \n",
        "    # Estad√≠sticas b√°sicas\n",
        "    total = len(cluster_data)\n",
        "    toxic_count = cluster_data['is_toxic'].sum()\n",
        "    toxic_pct = (toxic_count / total) * 100 if total > 0 else 0\n",
        "    \n",
        "    # Longitud promedio\n",
        "    avg_length = cluster_data[text_column].str.len().mean()\n",
        "    median_length = cluster_data[text_column].str.len().median()\n",
        "    \n",
        "    # Palabras promedio\n",
        "    avg_words = cluster_data[text_column].str.split().str.len().mean()\n",
        "    \n",
        "    # Palabras m√°s frecuentes\n",
        "    all_words = []\n",
        "    for text in cluster_data[text_column]:\n",
        "        if pd.notna(text):\n",
        "            words = str(text).lower().split()\n",
        "            all_words.extend(words)\n",
        "    \n",
        "    word_freq = Counter(all_words)\n",
        "    top_words = word_freq.most_common(10)\n",
        "    \n",
        "    return {\n",
        "        'total': total,\n",
        "        'toxic_count': toxic_count,\n",
        "        'toxic_pct': toxic_pct,\n",
        "        'avg_length': avg_length,\n",
        "        'median_length': median_length,\n",
        "        'avg_words': avg_words,\n",
        "        'top_words': top_words\n",
        "    }\n",
        "\n",
        "# Analizar cada cluster\n",
        "print(\"üìä An√°lisis Detallado de Clusters:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for cluster_id in sorted(df_analysis['cluster'].unique()):\n",
        "    stats = analyze_cluster(df_analysis, cluster_id)\n",
        "    print(f\"\\nüîπ Cluster {cluster_id}:\")\n",
        "    print(f\"   Total ejemplos: {stats['total']}\")\n",
        "    print(f\"   T√≥xicos: {stats['toxic_count']} ({stats['toxic_pct']:.1f}%)\")\n",
        "    print(f\"   Longitud promedio: {stats['avg_length']:.1f} caracteres\")\n",
        "    print(f\"   Longitud mediana: {stats['median_length']:.1f} caracteres\")\n",
        "    print(f\"   Palabras promedio: {stats['avg_words']:.1f}\")\n",
        "    print(f\"   Top 10 palabras:\")\n",
        "    for word, count in stats['top_words']:\n",
        "        print(f\"      '{word}': {count} veces\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualizar Diferencias entre Clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear visualizaciones comparativas\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Distribuci√≥n de t√≥xicos por cluster\n",
        "ax1 = axes[0, 0]\n",
        "cluster_toxic = df_analysis.groupby('cluster')['is_toxic'].agg(['sum', 'count'])\n",
        "cluster_toxic['pct'] = (cluster_toxic['sum'] / cluster_toxic['count']) * 100\n",
        "ax1.bar(cluster_toxic.index, cluster_toxic['pct'], color=['#EF4444', '#10B981'])\n",
        "ax1.set_xlabel('Cluster')\n",
        "ax1.set_ylabel('% T√≥xicos')\n",
        "ax1.set_title('Porcentaje de T√≥xicos por Cluster', fontweight='bold')\n",
        "ax1.set_ylim(0, 100)\n",
        "for i, (idx, row) in enumerate(cluster_toxic.iterrows()):\n",
        "    ax1.text(i, row['pct'] + 2, f\"{row['pct']:.1f}%\", ha='center', fontweight='bold')\n",
        "\n",
        "# 2. Longitud promedio por cluster\n",
        "ax2 = axes[0, 1]\n",
        "cluster_length = df_analysis.groupby('cluster')['Text'].apply(lambda x: x.str.len().mean())\n",
        "ax2.bar(cluster_length.index, cluster_length.values, color=['#3B82F6', '#8B5CF6'])\n",
        "ax2.set_xlabel('Cluster')\n",
        "ax2.set_ylabel('Longitud Promedio (caracteres)')\n",
        "ax2.set_title('Longitud Promedio por Cluster', fontweight='bold')\n",
        "for i, (idx, val) in enumerate(cluster_length.items()):\n",
        "    ax2.text(i, val + 5, f\"{val:.0f}\", ha='center', fontweight='bold')\n",
        "\n",
        "# 3. Distribuci√≥n de longitudes\n",
        "ax3 = axes[1, 0]\n",
        "for cluster_id in sorted(df_analysis['cluster'].unique()):\n",
        "    lengths = df_analysis[df_analysis['cluster'] == cluster_id]['Text'].str.len()\n",
        "    ax3.hist(lengths, alpha=0.6, label=f'Cluster {cluster_id}', bins=30)\n",
        "ax3.set_xlabel('Longitud (caracteres)')\n",
        "ax3.set_ylabel('Frecuencia')\n",
        "ax3.set_title('Distribuci√≥n de Longitudes por Cluster', fontweight='bold')\n",
        "ax3.legend()\n",
        "\n",
        "# 4. Matriz de confusi√≥n: Clusters vs Etiquetas Reales\n",
        "ax4 = axes[1, 1]\n",
        "crosstab = pd.crosstab(df_analysis['cluster'], df_analysis['is_toxic'])\n",
        "sns.heatmap(crosstab, annot=True, fmt='d', cmap='YlOrRd', ax=ax4, cbar_kws={'label': 'Cantidad'})\n",
        "ax4.set_xlabel('Es T√≥xico')\n",
        "ax4.set_ylabel('Cluster')\n",
        "ax4.set_title('Clusters vs Etiquetas Reales', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../data/processed/cluster_insights.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Gr√°ficos guardados en: ../data/processed/cluster_insights.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. An√°lisis de Outliers (DBSCAN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplicar DBSCAN para encontrar outliers\n",
        "pca_dbscan = PCA(n_components=50, random_state=42)\n",
        "X_reduced_dbscan = pca_dbscan.fit_transform(X_dense)\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(X_reduced_dbscan)\n",
        "\n",
        "# Identificar outliers\n",
        "outlier_indices = np.where(dbscan_labels == -1)[0]\n",
        "df_analysis['is_outlier'] = False\n",
        "df_analysis.loc[outlier_indices, 'is_outlier'] = True\n",
        "\n",
        "print(\"üìä An√°lisis de Outliers (DBSCAN):\")\n",
        "print(\"=\"*60)\n",
        "print(f\"   Total outliers: {len(outlier_indices)} ({len(outlier_indices)/len(df_analysis)*100:.1f}%)\")\n",
        "\n",
        "# Caracter√≠sticas de outliers\n",
        "outlier_data = df_analysis[df_analysis['is_outlier'] == True]\n",
        "if len(outlier_data) > 0:\n",
        "    print(f\"\\n   Caracter√≠sticas de outliers:\")\n",
        "    print(f\"   - T√≥xicos: {outlier_data['is_toxic'].sum()} ({outlier_data['is_toxic'].sum()/len(outlier_data)*100:.1f}%)\")\n",
        "    print(f\"   - Longitud promedio: {outlier_data['Text'].str.len().mean():.1f} caracteres\")\n",
        "    print(f\"   - Palabras promedio: {outlier_data['Text'].str.split().str.len().mean():.1f}\")\n",
        "    \n",
        "    # Mostrar algunos ejemplos\n",
        "    print(f\"\\n   Ejemplos de outliers:\")\n",
        "    for idx, row in outlier_data.head(5).iterrows():\n",
        "        toxic_label = \"T√ìXICO\" if row['is_toxic'] else \"NO T√ìXICO\"\n",
        "        print(f\"   [{toxic_label}] {str(row['Text'])[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Insights y Recomendaciones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üí° INSIGHTS Y RECOMENDACIONES:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Insight 1: Cluster peque√±o muy t√≥xico\n",
        "cluster_0 = df_analysis[df_analysis['cluster'] == 0]\n",
        "cluster_0_toxic_pct = (cluster_0['is_toxic'].sum() / len(cluster_0)) * 100\n",
        "\n",
        "if cluster_0_toxic_pct > 80:\n",
        "    print(\"\\n‚úÖ INSIGHT 1: Cluster Peque√±o Muy T√≥xico Detectado\")\n",
        "    print(f\"   - Cluster 0 tiene {len(cluster_0)} ejemplos ({len(cluster_0)/len(df_analysis)*100:.1f}% del dataset)\")\n",
        "    print(f\"   - {cluster_0_toxic_pct:.1f}% son t√≥xicos (muy alto)\")\n",
        "    print(\"   üìå RECOMENDACI√ìN:\")\n",
        "    print(\"      - Este cluster puede representar un tipo espec√≠fico de hate speech\")\n",
        "    print(\"      - Considerar crear features espec√≠ficas para este patr√≥n\")\n",
        "    print(\"      - Estos comentarios pueden ser f√°ciles de detectar por el modelo\")\n",
        "\n",
        "# Insight 2: An√°lisis de outliers\n",
        "if len(outlier_indices) > 0:\n",
        "    outlier_toxic_pct = (outlier_data['is_toxic'].sum() / len(outlier_data)) * 100\n",
        "    general_toxic_pct = (df_analysis['is_toxic'].sum() / len(df_analysis)) * 100\n",
        "    \n",
        "    print(\"\\n‚úÖ INSIGHT 2: Outliers Encontrados\")\n",
        "    print(f\"   - {len(outlier_indices)} outliers ({len(outlier_indices)/len(df_analysis)*100:.1f}%)\")\n",
        "    print(f\"   - {outlier_toxic_pct:.1f}% t√≥xicos (vs {general_toxic_pct:.1f}% general)\")\n",
        "    \n",
        "    if abs(outlier_toxic_pct - general_toxic_pct) > 10:\n",
        "        print(\"   üìå RECOMENDACI√ìN:\")\n",
        "        print(\"      - Los outliers tienen distribuci√≥n diferente de t√≥xicos\")\n",
        "        print(\"      - Revisar manualmente estos casos para entender por qu√© son at√≠picos\")\n",
        "        print(\"      - Pueden ser casos edge que el modelo tiene dificultad clasificando\")\n",
        "\n",
        "# Insight 3: Separabilidad\n",
        "ari = adjusted_rand_score(df_analysis['is_toxic'], df_analysis['cluster'])\n",
        "print(f\"\\n‚úÖ INSIGHT 3: Separabilidad de Clusters\")\n",
        "print(f\"   - Adjusted Rand Index: {ari:.4f}\")\n",
        "if ari > 0.1:\n",
        "    print(\"   üìå Los clusters se alinean parcialmente con etiquetas t√≥xicas/no t√≥xicas\")\n",
        "    print(\"   üìå RECOMENDACI√ìN:\")\n",
        "    print(\"      - Usar informaci√≥n de cluster como feature adicional en el modelo\")\n",
        "    print(\"      - Puede ayudar a mejorar la precisi√≥n\")\n",
        "else:\n",
        "    print(\"   üìå Los clusters no se alinean bien con etiquetas\")\n",
        "    print(\"   üìå RECOMENDACI√ìN:\")\n",
        "    print(\"      - Los clusters pueden representar subcategor√≠as dentro de t√≥xicos/no t√≥xicos\")\n",
        "    print(\"      - Investigar qu√© caracter√≠sticas definen cada cluster\")\n",
        "\n",
        "# Insight 4: Palabras caracter√≠sticas\n",
        "print(\"\\n‚úÖ INSIGHT 4: Palabras Caracter√≠sticas por Cluster\")\n",
        "for cluster_id in sorted(df_analysis['cluster'].unique()):\n",
        "    stats = analyze_cluster(df_analysis, cluster_id)\n",
        "    print(f\"\\n   Cluster {cluster_id} ({(stats['toxic_pct']):.1f}% t√≥xicos):\")\n",
        "    print(f\"      Top palabras: {', '.join([w for w, _ in stats['top_words'][:5]])}\")\n",
        "    \n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ An√°lisis de insights completado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Aplicaciones Pr√°cticas\n",
        "\n",
        "### Opci√≥n A: Usar Clusters como Feature Adicional\n",
        "\n",
        "Podemos a√±adir el cluster asignado como una feature adicional al modelo para mejorar la precisi√≥n.\n",
        "\n",
        "### Opci√≥n B: An√°lisis de Subcategor√≠as\n",
        "\n",
        "Los clusters pueden revelar subcategor√≠as de hate speech (racismo, misoginia, etc.) que pueden ser √∫tiles para an√°lisis m√°s detallado.\n",
        "\n",
        "### Opci√≥n C: Sampling Estratificado\n",
        "\n",
        "Usar informaci√≥n de clusters para hacer sampling m√°s inteligente en el entrenamiento, asegurando representaci√≥n de todos los tipos de comentarios.\n",
        "\n",
        "### Opci√≥n D: Detecci√≥n de Casos Edge\n",
        "\n",
        "Los outliers pueden ser casos dif√≠ciles que necesitan atenci√≥n especial o revisi√≥n manual.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
