{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Data Augmentation para Expansi√≥n del Dataset\n",
    "\n",
    "Este notebook implementa t√©cnicas de data augmentation para expandir el dataset y mejorar el rendimiento del modelo.\n",
    "\n",
    "## T√©cnicas implementadas:\n",
    "1. **Reemplazo por sin√≥nimos** (WordNet)\n",
    "2. **Traducci√≥n y back-translation** (googletrans)\n",
    "3. **Combinaci√≥n de t√©cnicas**\n",
    "\n",
    "## Objetivos:\n",
    "1. Expandir dataset de 1,000 a ~2,000-3,000 ejemplos\n",
    "2. Evaluar mejora en m√©tricas del modelo\n",
    "3. Comparar rendimiento antes/despu√©s de augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar librer√≠as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as importadas\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# A√±adir src al path\n",
    "sys.path.append(str(Path('../src').resolve()))\n",
    "\n",
    "# Descargar recursos de NLTK si no est√°n\n",
    "try:\n",
    "    import nltk\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from data.augmentation import TextAugmenter\n",
    "from data.preprocessing import TextPreprocessor\n",
    "from features.vectorization import TextVectorizer\n",
    "from models.train import train_model\n",
    "from models.evaluate import evaluate_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar dataset original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset original cargado:\n",
      "   Total: 1000 comentarios\n",
      "   T√≥xicos: 462\n",
      "   No t√≥xicos: 538\n",
      "\n",
      "   Columnas: ['CommentId', 'VideoId', 'Text', 'IsToxic', 'IsAbusive', 'IsThreat', 'IsProvocative', 'IsObscene', 'IsHatespeech', 'IsRacist', 'IsNationalist', 'IsSexist', 'IsHomophobic', 'IsReligiousHate', 'IsRadicalism']\n"
     ]
    }
   ],
   "source": [
    "# Cargar dataset original\n",
    "data_path = Path('../data/raw/youtoxic_english_1000.csv')\n",
    "df_original = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"‚úÖ Dataset original cargado:\")\n",
    "print(f\"   Total: {len(df_original)} comentarios\")\n",
    "print(f\"   T√≥xicos: {df_original['IsToxic'].sum()}\")\n",
    "print(f\"   No t√≥xicos: {len(df_original) - df_original['IsToxic'].sum()}\")\n",
    "print(f\"\\n   Columnas: {list(df_original.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocesar datos originales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ spaCy cargado: en_core_web_sm\n",
      "üîÑ Preprocesando texto original...\n",
      "‚úÖ Preprocesamiento completado\n",
      "   Ejemplo de texto procesado:\n",
      "   Original: If only people would just take a step back and not make this case about them, because it wasn't abou...\n",
      "   Procesado: people step case wasn t people situation lump mess matter hand make kind protest selfish rational th...\n"
     ]
    }
   ],
   "source": [
    "# Preprocesar texto original\n",
    "preprocessor = TextPreprocessor(use_spacy=True)\n",
    "\n",
    "print(\"üîÑ Preprocesando texto original...\")\n",
    "df_original['Text_processed'] = df_original['Text'].apply(\n",
    "    lambda x: preprocessor.preprocess_text(str(x), remove_stopwords=True)\n",
    ")\n",
    "\n",
    "# Preparar columnas para augmentation\n",
    "df_for_aug = df_original[['Text_processed', 'IsToxic']].copy()\n",
    "df_for_aug.columns = ['text', 'label']\n",
    "\n",
    "print(f\"‚úÖ Preprocesamiento completado\")\n",
    "print(f\"   Ejemplo de texto procesado:\")\n",
    "print(f\"   Original: {df_original['Text'].iloc[0][:100]}...\")\n",
    "print(f\"   Procesado: {df_for_aug['text'].iloc[0][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inicializar Augmenter y Probar T√©cnicas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TextAugmenter inicializado\n",
      "   Traducci√≥n disponible: False\n",
      "   Sin√≥nimos disponibles: True\n",
      "\n",
      "üìù Texto original:\n",
      "   people step case wasn t people situation lump mess matter hand make kind protest selfish rational thought investigation guy video heavily emotional hype want hear get hear press reasonable discussion kudo smerconish keep level time let masri fool dare tear city protest dishonor entire incident hate way police brutality epidemic wish stop pretend like know exactly go s measurable people honestly witness incident clue way issue swing grand jury informed trust majority rule right course action let thank 99 99 police officer america actually serve protect bit jerk pull respect job know people go pout hold accountable action people hate police need officer emergency\n",
      "\n",
      "üîÑ Con sin√≥nimos:\n",
      "   people step face wasn t people site lump mess matter paw cook kind dissent selfish rational thought investigation guy tv heavily emotional hype want learn get hear press sensible discourse kudo smerconish keep level time let masri fool dare tear city protest dishonor total incident hate way patrol brutality epidemic care plosive pretend same fuck exactly go s measurable multitude honestly witness incidental clue way takings swing grand jury informed trust bulk govern right course process let thank 99 99 police officer u.s. really serve protect bit jerk pull respect job fuck people go pout hold accountable process people hatred police need officer exigency\n"
     ]
    }
   ],
   "source": [
    "# Inicializar augmenter\n",
    "augmenter = TextAugmenter(use_translation=True, use_synonyms=True)\n",
    "\n",
    "print(\"‚úÖ TextAugmenter inicializado\")\n",
    "print(f\"   Traducci√≥n disponible: {augmenter.use_translation}\")\n",
    "print(f\"   Sin√≥nimos disponibles: {augmenter.use_synonyms}\")\n",
    "\n",
    "# Probar con un ejemplo\n",
    "test_text = df_for_aug['text'].iloc[0]\n",
    "print(f\"\\nüìù Texto original:\")\n",
    "print(f\"   {test_text}\")\n",
    "\n",
    "# Probar sin√≥nimos\n",
    "if augmenter.use_synonyms:\n",
    "    augmented_synonyms = augmenter.replace_with_synonyms(test_text, replacement_ratio=0.3)\n",
    "    print(f\"\\nüîÑ Con sin√≥nimos:\")\n",
    "    print(f\"   {augmented_synonyms}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aumentar Dataset Completo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Aumentando dataset...\n",
      "   Esto puede tardar varios minutos...\n",
      "\n",
      "============================================================\n",
      "üîÑ Aumentando dataset: 1000 ‚Üí 2000 ejemplos\n",
      "   M√©todos: ['synonyms']\n",
      "‚úÖ Dataset aumentado: 1917 ejemplos totales\n",
      "   Originales: 1000\n",
      "   Aumentados: 917\n",
      "\n",
      "============================================================\n",
      "‚úÖ Dataset aumentado:\n",
      "   Original: 1000 ejemplos\n",
      "   Aumentado: 1917 ejemplos\n",
      "   Incremento: 917 ejemplos (91.7%)\n"
     ]
    }
   ],
   "source": [
    "# Aumentar dataset (duplicar tama√±o: augmentation_factor=1.0)\n",
    "print(\"üîÑ Aumentando dataset...\")\n",
    "print(\"   Esto puede tardar varios minutos...\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Usar solo sin√≥nimos para ser m√°s r√°pido (traducci√≥n es muy lenta)\n",
    "methods = ['synonyms']\n",
    "if augmenter.use_translation:\n",
    "    # Opcional: a√±adir traducci√≥n (muy lento)\n",
    "    # methods.append('translation')\n",
    "    pass\n",
    "\n",
    "df_augmented = augmenter.augment_dataframe(\n",
    "    df_for_aug,\n",
    "    text_column='text',\n",
    "    label_column='label',\n",
    "    augmentation_factor=1.0,  # Duplicar dataset\n",
    "    methods=methods\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ Dataset aumentado:\")\n",
    "print(f\"   Original: {len(df_for_aug)} ejemplos\")\n",
    "print(f\"   Aumentado: {len(df_augmented)} ejemplos\")\n",
    "print(f\"   Incremento: {len(df_augmented) - len(df_for_aug)} ejemplos ({((len(df_augmented) - len(df_for_aug))/len(df_for_aug)*100):.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Guardar Dataset Aumentado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset aumentado guardado en: ../data/processed/youtoxic_english_1000_augmented.csv\n",
      "\n",
      "   Estad√≠sticas:\n",
      "   Total: 1917 ejemplos\n",
      "   Originales: 1000\n",
      "   Aumentados: 917\n",
      "   T√≥xicos: 904\n",
      "   No t√≥xicos: 1013\n"
     ]
    }
   ],
   "source": [
    "# Guardar dataset aumentado\n",
    "output_path = Path('../data/processed/youtoxic_english_1000_augmented.csv')\n",
    "\n",
    "# Preparar para guardar (usar nombres de columnas originales)\n",
    "df_to_save = df_augmented.copy()\n",
    "df_to_save['Text'] = df_to_save['text']\n",
    "df_to_save['IsToxic'] = df_to_save['label'].astype(int)\n",
    "\n",
    "# Guardar solo columnas necesarias\n",
    "df_to_save[['Text', 'IsToxic', '_augmented', '_augmentation_method']].to_csv(\n",
    "    output_path,\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset aumentado guardado en: {output_path}\")\n",
    "print(f\"\\n   Estad√≠sticas:\")\n",
    "print(f\"   Total: {len(df_to_save)} ejemplos\")\n",
    "print(f\"   Originales: {len(df_to_save[~df_to_save['_augmented']])}\")\n",
    "print(f\"   Aumentados: {len(df_to_save[df_to_save['_augmented']])}\")\n",
    "print(f\"   T√≥xicos: {df_to_save['IsToxic'].sum()}\")\n",
    "print(f\"   No t√≥xicos: {len(df_to_save) - df_to_save['IsToxic'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Vectorizar y Entrenar Modelo con Dataset Aumentado (Opcional)\n",
    "\n",
    "> **Nota**: Esta secci√≥n es opcional. Puedes entrenar el modelo con el dataset aumentado y comparar m√©tricas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Vectorizando dataset aumentado...\n",
      "‚úÖ Vectorizaci√≥n completada:\n",
      "   Train: (1533, 5000)\n",
      "   Test: (384, 5000)\n",
      "   Features: 5000\n"
     ]
    }
   ],
   "source": [
    "# Vectorizar dataset aumentado\n",
    "print(\"üîÑ Vectorizando dataset aumentado...\")\n",
    "\n",
    "# Preparar datos\n",
    "X_aug = df_to_save['Text'].values\n",
    "y_aug = df_to_save['IsToxic'].values\n",
    "\n",
    "# Crear vectorizador y entrenar\n",
    "vectorizer = TextVectorizer(method='tfidf', max_features=5000)\n",
    "X_aug_vectorized = vectorizer.fit_transform(pd.Series(X_aug))\n",
    "\n",
    "# Split train/test\n",
    "X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(\n",
    "    X_aug_vectorized,\n",
    "    y_aug,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_aug\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Vectorizaci√≥n completada:\")\n",
    "print(f\"   Train: {X_train_aug.shape}\")\n",
    "print(f\"   Test: {X_test_aug.shape}\")\n",
    "print(f\"   Features: {X_train_aug.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entrenar Modelo con Dataset Aumentado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Entrenando modelo SVM con dataset aumentado...\n",
      "============================================================\n",
      "‚úÖ Modelo entrenado con dataset aumentado\n",
      "================================================================================\n",
      "RESULTADOS DE EVALUACI√ìN\n",
      "================================================================================\n",
      "\n",
      "üìä M√âTRICAS EN TRAIN:\n",
      "   Accuracy:  0.8969\n",
      "   Precision: 0.8929\n",
      "   Recall:    0.8880\n",
      "   F1-score:  0.8904\n",
      "\n",
      "üìä M√âTRICAS EN TEST:\n",
      "   Accuracy:  0.8490\n",
      "   Precision: 0.8773\n",
      "   Recall:    0.7901\n",
      "   F1-score:  0.8314\n",
      "\n",
      "‚ö†Ô∏è  OVERFITTING:\n",
      "   Diferencia F1 (train-test): 5.90%\n",
      "   ‚ö†Ô∏è  Overfitting moderado (5-10%)\n",
      "\n",
      "üìã Matriz de confusi√≥n (test):\n",
      "[[183  20]\n",
      " [ 38 143]]\n"
     ]
    }
   ],
   "source": [
    "# Entrenar SVM con dataset aumentado usando mismos par√°metros optimizados\n",
    "print(\"üîÑ Entrenando modelo SVM con dataset aumentado...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Usar par√°metros optimizados del modelo original\n",
    "svm_aug = SVC(C=0.056, kernel='linear', probability=True, class_weight='balanced', random_state=42)\n",
    "svm_aug_calibrated = CalibratedClassifierCV(svm_aug, method='sigmoid', cv=3)\n",
    "svm_aug_calibrated.fit(X_train_aug, y_train_aug)\n",
    "\n",
    "print(\"‚úÖ Modelo entrenado con dataset aumentado\")\n",
    "\n",
    "# Evaluar modelo aumentado\n",
    "results_aug = evaluate_model(\n",
    "    svm_aug_calibrated,\n",
    "    X_train_aug,\n",
    "    X_test_aug,\n",
    "    pd.Series(y_train_aug),\n",
    "    pd.Series(y_test_aug),\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cargar y Evaluar Modelo Original para Comparar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Cargando datos originales para comparaci√≥n...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_vectorized_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cargar datos originales vectorizados para comparar\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîÑ Cargando datos originales para comparaci√≥n...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m X_train_orig, X_test_orig, y_train_orig, y_test_orig = \u001b[43mload_vectorized_data\u001b[49m(data_dir, prefix=\u001b[33m'\u001b[39m\u001b[33mtfidf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Cargar modelo original optimizado\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'load_vectorized_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Cargar datos originales vectorizados para comparar\n",
    "print(\"üîÑ Cargando datos originales para comparaci√≥n...\")\n",
    "from features.vectorization import load_vectorized_data\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = load_vectorized_data(data_dir, prefix='tfidf')\n",
    "\n",
    "# Cargar modelo original optimizado\n",
    "import pickle\n",
    "model_path = Path('../models/optimized/best_optimized_model.pkl')\n",
    "if model_path.exists():\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model_original = pickle.load(f)\n",
    "    print(\"‚úÖ Modelo original cargado\")\n",
    "    \n",
    "    # Evaluar modelo original\n",
    "    results_original = evaluate_model(\n",
    "        model_original,\n",
    "        X_train_orig,\n",
    "        X_test_orig,\n",
    "        y_train_orig,\n",
    "        y_test_orig,\n",
    "        verbose=True\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Modelo original no encontrado. Usando m√©tricas conocidas.\")\n",
    "    # M√©tricas del modelo original (SVM optimizado)\n",
    "    results_original = {\n",
    "        'test_f1': 0.7407,\n",
    "        'test_accuracy': 0.64,\n",
    "        'test_precision': 0.6452,\n",
    "        'test_recall': 0.8696,\n",
    "        'train_f1': 0.7119,\n",
    "        'diff_f1': 2.54\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparaci√≥n Detallada: Original vs Aumentado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame comparativo\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä COMPARACI√ìN: Modelo Original vs Modelo con Augmentation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_data = {\n",
    "    'M√©trica': ['F1-Score (Test)', 'Accuracy (Test)', 'Precision (Test)', 'Recall (Test)', 'Overfitting (%)'],\n",
    "    'Original': [\n",
    "        results_original['test_f1'],\n",
    "        results_original['test_accuracy'],\n",
    "        results_original['test_precision'],\n",
    "        results_original['test_recall'],\n",
    "        results_original['diff_f1']\n",
    "    ],\n",
    "    'Con Augmentation': [\n",
    "        results_aug['test_f1'],\n",
    "        results_aug['test_accuracy'],\n",
    "        results_aug['test_precision'],\n",
    "        results_aug['test_recall'],\n",
    "        results_aug['diff_f1']\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df['Mejora'] = comparison_df['Con Augmentation'] - comparison_df['Original']\n",
    "comparison_df['Mejora %'] = ((comparison_df['Con Augmentation'] - comparison_df['Original']) / comparison_df['Original'] * 100).round(2)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Determinar si hay mejora\n",
    "f1_improvement = comparison_df[comparison_df['M√©trica'] == 'F1-Score (Test)']['Mejora'].values[0]\n",
    "overfitting_change = comparison_df[comparison_df['M√©trica'] == 'Overfitting (%)']['Mejora'].values[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if f1_improvement > 0.01:  # Mejora significativa (>1%)\n",
    "    print(\"‚úÖ RESULTADO: Data Augmentation MEJORA el modelo\")\n",
    "    print(f\"   - F1-Score mejor√≥ en {f1_improvement:.4f} ({comparison_df[comparison_df['M√©trica'] == 'F1-Score (Test)']['Mejora %'].values[0]:.2f}%)\")\n",
    "elif f1_improvement < -0.01:  # Empeoramiento significativo\n",
    "    print(\"‚ùå RESULTADO: Data Augmentation EMPEORA el modelo\")\n",
    "    print(f\"   - F1-Score empeor√≥ en {abs(f1_improvement):.4f}\")\n",
    "else:\n",
    "    print(\"‚ûñ RESULTADO: Data Augmentation no cambia significativamente el modelo\")\n",
    "    print(f\"   - F1-Score cambi√≥ en {f1_improvement:.4f}\")\n",
    "\n",
    "if abs(overfitting_change) > 1:\n",
    "    if overfitting_change > 0:\n",
    "        print(f\"   ‚ö†Ô∏è  Overfitting aument√≥ en {overfitting_change:.2f}%\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Overfitting disminuy√≥ en {abs(overfitting_change):.2f}%\")\n",
    "else:\n",
    "    print(f\"   ‚ûñ Overfitting se mantiene similar ({overfitting_change:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualizaci√≥n de Comparaci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# M√©tricas principales\n",
    "metrics = ['F1-Score (Test)', 'Accuracy (Test)', 'Precision (Test)', 'Recall (Test)']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.bar(x - width/2, [comparison_df[comparison_df['M√©trica'] == m]['Original'].values[0] for m in metrics], \n",
    "        width, label='Original', color='steelblue')\n",
    "ax1.bar(x + width/2, [comparison_df[comparison_df['M√©trica'] == m]['Con Augmentation'].values[0] for m in metrics], \n",
    "        width, label='Con Augmentation', color='coral')\n",
    "ax1.set_xlabel('M√©tricas')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Comparaci√≥n de M√©tricas', fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Overfitting\n",
    "ax2 = axes[1]\n",
    "overfitting_orig = comparison_df[comparison_df['M√©trica'] == 'Overfitting (%)']['Original'].values[0]\n",
    "overfitting_aug = comparison_df[comparison_df['M√©trica'] == 'Overfitting (%)']['Con Augmentation'].values[0]\n",
    "ax2.bar(['Original', 'Con Augmentation'], \n",
    "        [overfitting_orig, overfitting_aug],\n",
    "        color=['steelblue', 'coral'])\n",
    "ax2.axhline(y=5, color='r', linestyle='--', label='L√≠mite objetivo (5%)')\n",
    "ax2.set_ylabel('Overfitting (%)')\n",
    "ax2.set_title('Overfitting', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/augmentation_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Gr√°ficos guardados en: ../data/processed/augmentation_comparison.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Guardar Modelo Aumentado (si mejora)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo aumentado si mejora significativamente\n",
    "f1_improvement = comparison_df[comparison_df['M√©trica'] == 'F1-Score (Test)']['Mejora'].values[0]\n",
    "\n",
    "if f1_improvement > 0.01:  # Mejora > 1%\n",
    "    models_dir = Path('../models/augmented')\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model_path = models_dir / 'svm_augmented_model.pkl'\n",
    "    vectorizer_path = models_dir / 'tfidf_vectorizer_augmented.pkl'\n",
    "    \n",
    "    # Guardar modelo\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(svm_aug_calibrated, f)\n",
    "    \n",
    "    # Guardar vectorizador\n",
    "    with open(vectorizer_path, 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    \n",
    "    # Guardar informaci√≥n del modelo\n",
    "    model_info = {\n",
    "        'model_name': 'SVM (Augmented)',\n",
    "        'vectorizer_type': 'tfidf',\n",
    "        'test_f1': results_aug['test_f1'],\n",
    "        'test_accuracy': results_aug['test_accuracy'],\n",
    "        'overfitting': results_aug['diff_f1'],\n",
    "        'train_f1': results_aug['train_f1'],\n",
    "        'dataset_size': len(df_augmented),\n",
    "        'augmentation_method': 'synonyms'\n",
    "    }\n",
    "    \n",
    "    info_path = models_dir / 'svm_augmented_model_info.pkl'\n",
    "    with open(info_path, 'wb') as f:\n",
    "        pickle.dump(model_info, f)\n",
    "    \n",
    "    print(f\"‚úÖ Modelo aumentado guardado:\")\n",
    "    print(f\"   Modelo: {model_path}\")\n",
    "    print(f\"   Vectorizador: {vectorizer_path}\")\n",
    "    print(f\"   Info: {info_path}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Modelo aumentado no se guarda (no hay mejora significativa)\")\n",
    "    print(\"   El modelo original sigue siendo el mejor\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Conclusiones Finales\n",
    "\n",
    "### Resultados del Data Augmentation:\n",
    "\n",
    "1. **Tama√±o del dataset**: ‚úÖ Expandido de 1,000 a ~1,900 ejemplos\n",
    "2. **Balance de clases**: Verificar si se mantiene\n",
    "3. **Mejora en m√©tricas**: Ver comparaci√≥n arriba\n",
    "\n",
    "### Recomendaciones:\n",
    "\n",
    "- **Si mejora**: Considerar usar modelo aumentado en producci√≥n\n",
    "- **Si no mejora**: El dataset original es suficiente, augmentation no aporta valor\n",
    "- **Pr√≥ximos pasos**: Probar otras t√©cnicas (traducci√≥n, parafraseo) o aumentar solo clase minoritaria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aumentar Dataset Completo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
