{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ”„ Data Augmentation para ExpansiÃ³n del Dataset\n",
        "\n",
        "Este notebook implementa tÃ©cnicas de data augmentation para expandir el dataset y mejorar el rendimiento del modelo.\n",
        "\n",
        "## TÃ©cnicas implementadas:\n",
        "1. **Reemplazo por sinÃ³nimos** (WordNet)\n",
        "2. **TraducciÃ³n y back-translation** (googletrans)\n",
        "3. **CombinaciÃ³n de tÃ©cnicas**\n",
        "\n",
        "## Objetivos:\n",
        "1. Expandir dataset de 1,000 a ~2,000-3,000 ejemplos\n",
        "2. Evaluar mejora en mÃ©tricas del modelo\n",
        "3. Comparar rendimiento antes/despuÃ©s de augmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importar librerÃ­as\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# AÃ±adir src al path\n",
        "sys.path.append(str(Path('../src').resolve()))\n",
        "\n",
        "# Descargar recursos de NLTK si no estÃ¡n\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    nltk.download('omw-1.4', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "from data.augmentation import TextAugmenter\n",
        "from data.preprocessing import TextPreprocessor\n",
        "from features.vectorization import TextVectorizer\n",
        "from models.train import train_model\n",
        "from models.evaluate import evaluate_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"âœ… LibrerÃ­as importadas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cargar dataset original\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar dataset original\n",
        "data_path = Path('../data/raw/youtoxic_english_1000.csv')\n",
        "df_original = pd.read_csv(data_path)\n",
        "\n",
        "print(f\"âœ… Dataset original cargado:\")\n",
        "print(f\"   Total: {len(df_original)} comentarios\")\n",
        "print(f\"   TÃ³xicos: {df_original['IsToxic'].sum()}\")\n",
        "print(f\"   No tÃ³xicos: {len(df_original) - df_original['IsToxic'].sum()}\")\n",
        "print(f\"\\n   Columnas: {list(df_original.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocesar datos originales\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocesar texto original\n",
        "preprocessor = TextPreprocessor(use_spacy=True)\n",
        "\n",
        "print(\"ðŸ”„ Preprocesando texto original...\")\n",
        "df_original['Text_processed'] = df_original['Text'].apply(\n",
        "    lambda x: preprocessor.preprocess_text(str(x), remove_stopwords=True)\n",
        ")\n",
        "\n",
        "# Preparar columnas para augmentation\n",
        "df_for_aug = df_original[['Text_processed', 'IsToxic']].copy()\n",
        "df_for_aug.columns = ['text', 'label']\n",
        "\n",
        "print(f\"âœ… Preprocesamiento completado\")\n",
        "print(f\"   Ejemplo de texto procesado:\")\n",
        "print(f\"   Original: {df_original['Text'].iloc[0][:100]}...\")\n",
        "print(f\"   Procesado: {df_for_aug['text'].iloc[0][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Inicializar Augmenter y Probar TÃ©cnicas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicializar augmenter\n",
        "augmenter = TextAugmenter(use_translation=True, use_synonyms=True)\n",
        "\n",
        "print(\"âœ… TextAugmenter inicializado\")\n",
        "print(f\"   TraducciÃ³n disponible: {augmenter.use_translation}\")\n",
        "print(f\"   SinÃ³nimos disponibles: {augmenter.use_synonyms}\")\n",
        "\n",
        "# Probar con un ejemplo\n",
        "test_text = df_for_aug['text'].iloc[0]\n",
        "print(f\"\\nðŸ“ Texto original:\")\n",
        "print(f\"   {test_text}\")\n",
        "\n",
        "# Probar sinÃ³nimos\n",
        "if augmenter.use_synonyms:\n",
        "    augmented_synonyms = augmenter.replace_with_synonyms(test_text, replacement_ratio=0.3)\n",
        "    print(f\"\\nðŸ”„ Con sinÃ³nimos:\")\n",
        "    print(f\"   {augmented_synonyms}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Aumentar Dataset Completo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aumentar dataset (duplicar tamaÃ±o: augmentation_factor=1.0)\n",
        "print(\"ðŸ”„ Aumentando dataset...\")\n",
        "print(\"   Esto puede tardar varios minutos...\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Usar solo sinÃ³nimos para ser mÃ¡s rÃ¡pido (traducciÃ³n es muy lenta)\n",
        "methods = ['synonyms']\n",
        "if augmenter.use_translation:\n",
        "    # Opcional: aÃ±adir traducciÃ³n (muy lento)\n",
        "    # methods.append('translation')\n",
        "    pass\n",
        "\n",
        "df_augmented = augmenter.augment_dataframe(\n",
        "    df_for_aug,\n",
        "    text_column='text',\n",
        "    label_column='label',\n",
        "    augmentation_factor=1.0,  # Duplicar dataset\n",
        "    methods=methods\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"âœ… Dataset aumentado:\")\n",
        "print(f\"   Original: {len(df_for_aug)} ejemplos\")\n",
        "print(f\"   Aumentado: {len(df_augmented)} ejemplos\")\n",
        "print(f\"   Incremento: {len(df_augmented) - len(df_for_aug)} ejemplos ({((len(df_augmented) - len(df_for_aug))/len(df_for_aug)*100):.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Guardar Dataset Aumentado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar dataset aumentado\n",
        "output_path = Path('../data/processed/youtoxic_english_1000_augmented.csv')\n",
        "\n",
        "# Preparar para guardar (usar nombres de columnas originales)\n",
        "df_to_save = df_augmented.copy()\n",
        "df_to_save['Text'] = df_to_save['text']\n",
        "df_to_save['IsToxic'] = df_to_save['label'].astype(int)\n",
        "\n",
        "# Guardar solo columnas necesarias\n",
        "df_to_save[['Text', 'IsToxic', '_augmented', '_augmentation_method']].to_csv(\n",
        "    output_path,\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(f\"âœ… Dataset aumentado guardado en: {output_path}\")\n",
        "print(f\"\\n   EstadÃ­sticas:\")\n",
        "print(f\"   Total: {len(df_to_save)} ejemplos\")\n",
        "print(f\"   Originales: {len(df_to_save[~df_to_save['_augmented']])}\")\n",
        "print(f\"   Aumentados: {len(df_to_save[df_to_save['_augmented']])}\")\n",
        "print(f\"   TÃ³xicos: {df_to_save['IsToxic'].sum()}\")\n",
        "print(f\"   No tÃ³xicos: {len(df_to_save) - df_to_save['IsToxic'].sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Vectorizar y Entrenar Modelo con Dataset Aumentado (Opcional)\n",
        "\n",
        "> **Nota**: Esta secciÃ³n es opcional. Puedes entrenar el modelo con el dataset aumentado y comparar mÃ©tricas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vectorizar dataset aumentado\n",
        "print(\"ðŸ”„ Vectorizando dataset aumentado...\")\n",
        "\n",
        "# Preparar datos\n",
        "X_aug = df_to_save['Text'].values\n",
        "y_aug = df_to_save['IsToxic'].values\n",
        "\n",
        "# Crear vectorizador y entrenar\n",
        "vectorizer = TextVectorizer(method='tfidf', max_features=5000)\n",
        "X_aug_vectorized = vectorizer.fit_transform(pd.Series(X_aug))\n",
        "\n",
        "# Split train/test\n",
        "X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(\n",
        "    X_aug_vectorized,\n",
        "    y_aug,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_aug\n",
        ")\n",
        "\n",
        "print(f\"âœ… VectorizaciÃ³n completada:\")\n",
        "print(f\"   Train: {X_train_aug.shape}\")\n",
        "print(f\"   Test: {X_test_aug.shape}\")\n",
        "print(f\"   Features: {X_train_aug.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Conclusiones\n",
        "\n",
        "### Resultados del Data Augmentation:\n",
        "\n",
        "1. **TamaÃ±o del dataset**: Â¿Se duplicÃ³ correctamente?\n",
        "2. **Balance de clases**: Â¿Se mantiene el balance?\n",
        "3. **Calidad del texto aumentado**: Â¿Los sinÃ³nimos son apropiados?\n",
        "\n",
        "### Recomendaciones:\n",
        "\n",
        "- Si mejora mÃ©tricas: Usar dataset aumentado para entrenar modelo final\n",
        "- Si no mejora: Investigar otras tÃ©cnicas de augmentation\n",
        "- Considerar: Aumentar solo la clase minoritaria para balancear\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Aumentar Dataset Completo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
