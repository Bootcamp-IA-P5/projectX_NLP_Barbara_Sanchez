{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”„ Data Augmentation para ExpansiÃ³n del Dataset\n",
    "\n",
    "Este notebook implementa tÃ©cnicas de data augmentation para expandir el dataset y mejorar el rendimiento del modelo.\n",
    "\n",
    "## TÃ©cnicas implementadas:\n",
    "1. **Reemplazo por sinÃ³nimos** (WordNet)\n",
    "2. **TraducciÃ³n y back-translation** (googletrans)\n",
    "3. **CombinaciÃ³n de tÃ©cnicas**\n",
    "\n",
    "## Objetivos:\n",
    "1. Expandir dataset de 1,000 a ~2,000-3,000 ejemplos\n",
    "2. Evaluar mejora en mÃ©tricas del modelo\n",
    "3. Comparar rendimiento antes/despuÃ©s de augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar librerÃ­as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  googletrans no disponible. TraducciÃ³n no funcionarÃ¡.\n",
      "âœ… LibrerÃ­as importadas\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# AÃ±adir src al path\n",
    "sys.path.append(str(Path('../src').resolve()))\n",
    "\n",
    "# Descargar recursos de NLTK si no estÃ¡n\n",
    "try:\n",
    "    import nltk\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from data.augmentation import TextAugmenter\n",
    "from data.preprocessing import TextPreprocessor\n",
    "from features.vectorization import TextVectorizer\n",
    "from models.train import train_model\n",
    "from models.evaluate import evaluate_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"âœ… LibrerÃ­as importadas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar dataset original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset original cargado:\n",
      "   Total: 1000 comentarios\n",
      "   TÃ³xicos: 462\n",
      "   No tÃ³xicos: 538\n",
      "\n",
      "   Columnas: ['CommentId', 'VideoId', 'Text', 'IsToxic', 'IsAbusive', 'IsThreat', 'IsProvocative', 'IsObscene', 'IsHatespeech', 'IsRacist', 'IsNationalist', 'IsSexist', 'IsHomophobic', 'IsReligiousHate', 'IsRadicalism']\n"
     ]
    }
   ],
   "source": [
    "# Cargar dataset original\n",
    "data_path = Path('../data/raw/youtoxic_english_1000.csv')\n",
    "df_original = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"âœ… Dataset original cargado:\")\n",
    "print(f\"   Total: {len(df_original)} comentarios\")\n",
    "print(f\"   TÃ³xicos: {df_original['IsToxic'].sum()}\")\n",
    "print(f\"   No tÃ³xicos: {len(df_original) - df_original['IsToxic'].sum()}\")\n",
    "print(f\"\\n   Columnas: {list(df_original.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocesar datos originales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… spaCy cargado: en_core_web_sm\n",
      "ðŸ”„ Preprocesando texto original...\n",
      "âœ… Preprocesamiento completado\n",
      "   Ejemplo de texto procesado:\n",
      "   Original: If only people would just take a step back and not make this case about them, because it wasn't abou...\n",
      "   Procesado: people step case wasn t people situation lump mess matter hand make kind protest selfish rational th...\n"
     ]
    }
   ],
   "source": [
    "# Preprocesar texto original\n",
    "preprocessor = TextPreprocessor(use_spacy=True)\n",
    "\n",
    "print(\"ðŸ”„ Preprocesando texto original...\")\n",
    "df_original['Text_processed'] = df_original['Text'].apply(\n",
    "    lambda x: preprocessor.preprocess_text(str(x), remove_stopwords=True)\n",
    ")\n",
    "\n",
    "# Preparar columnas para augmentation\n",
    "df_for_aug = df_original[['Text_processed', 'IsToxic']].copy()\n",
    "df_for_aug.columns = ['text', 'label']\n",
    "\n",
    "print(f\"âœ… Preprocesamiento completado\")\n",
    "print(f\"   Ejemplo de texto procesado:\")\n",
    "print(f\"   Original: {df_original['Text'].iloc[0][:100]}...\")\n",
    "print(f\"   Procesado: {df_for_aug['text'].iloc[0][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inicializar Augmenter y Probar TÃ©cnicas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TextAugmenter inicializado\n",
      "   TraducciÃ³n disponible: False\n",
      "   SinÃ³nimos disponibles: True\n",
      "\n",
      "ðŸ“ Texto original:\n",
      "   people step case wasn t people situation lump mess matter hand make kind protest selfish rational thought investigation guy video heavily emotional hype want hear get hear press reasonable discussion kudo smerconish keep level time let masri fool dare tear city protest dishonor entire incident hate way police brutality epidemic wish stop pretend like know exactly go s measurable people honestly witness incident clue way issue swing grand jury informed trust majority rule right course action let thank 99 99 police officer america actually serve protect bit jerk pull respect job know people go pout hold accountable action people hate police need officer emergency\n",
      "\n",
      "ðŸ”„ Con sinÃ³nimos:\n",
      "   people step face wasn t multitude situation lump mess matter hand make kind protest selfish rational sentiment investigation guy video heavily emotional ballyhoo want learn get learn imperativeness reasonable discourse kudo smerconish keep stratum time let masri fool dare tear metropolis protest ravish total incident hate way police barbarity epidemic wish stop pretend like fuck precisely go s measurable multitude honestly watcher incident clue way issue swing grand panel informed cartel majority rule right course action let thank 99 99 police officer u.s. actually serve protect bit jerk deplumate honour job fuck people go moue hold accountable action multitude hatred police need officeholder emergency\n"
     ]
    }
   ],
   "source": [
    "# Inicializar augmenter\n",
    "augmenter = TextAugmenter(use_translation=True, use_synonyms=True)\n",
    "\n",
    "print(\"âœ… TextAugmenter inicializado\")\n",
    "print(f\"   TraducciÃ³n disponible: {augmenter.use_translation}\")\n",
    "print(f\"   SinÃ³nimos disponibles: {augmenter.use_synonyms}\")\n",
    "\n",
    "# Probar con un ejemplo\n",
    "test_text = df_for_aug['text'].iloc[0]\n",
    "print(f\"\\nðŸ“ Texto original:\")\n",
    "print(f\"   {test_text}\")\n",
    "\n",
    "# Probar sinÃ³nimos\n",
    "if augmenter.use_synonyms:\n",
    "    augmented_synonyms = augmenter.replace_with_synonyms(test_text, replacement_ratio=0.3)\n",
    "    print(f\"\\nðŸ”„ Con sinÃ³nimos:\")\n",
    "    print(f\"   {augmented_synonyms}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aumentar Dataset Completo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Aumentando dataset...\n",
      "   Esto puede tardar varios minutos...\n",
      "\n",
      "============================================================\n",
      "ðŸ”„ Aumentando dataset: 1000 â†’ 2000 ejemplos\n",
      "   MÃ©todos: ['synonyms']\n",
      "âœ… Dataset aumentado: 1909 ejemplos totales\n",
      "   Originales: 1000\n",
      "   Aumentados: 909\n",
      "\n",
      "============================================================\n",
      "âœ… Dataset aumentado:\n",
      "   Original: 1000 ejemplos\n",
      "   Aumentado: 1909 ejemplos\n",
      "   Incremento: 909 ejemplos (90.9%)\n"
     ]
    }
   ],
   "source": [
    "# Aumentar dataset (duplicar tamaÃ±o: augmentation_factor=1.0)\n",
    "print(\"ðŸ”„ Aumentando dataset...\")\n",
    "print(\"   Esto puede tardar varios minutos...\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Usar solo sinÃ³nimos para ser mÃ¡s rÃ¡pido (traducciÃ³n es muy lenta)\n",
    "methods = ['synonyms']\n",
    "if augmenter.use_translation:\n",
    "    # Opcional: aÃ±adir traducciÃ³n (muy lento)\n",
    "    # methods.append('translation')\n",
    "    pass\n",
    "\n",
    "df_augmented = augmenter.augment_dataframe(\n",
    "    df_for_aug,\n",
    "    text_column='text',\n",
    "    label_column='label',\n",
    "    augmentation_factor=1.0,  # Duplicar dataset\n",
    "    methods=methods\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"âœ… Dataset aumentado:\")\n",
    "print(f\"   Original: {len(df_for_aug)} ejemplos\")\n",
    "print(f\"   Aumentado: {len(df_augmented)} ejemplos\")\n",
    "print(f\"   Incremento: {len(df_augmented) - len(df_for_aug)} ejemplos ({((len(df_augmented) - len(df_for_aug))/len(df_for_aug)*100):.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Guardar Dataset Aumentado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset aumentado guardado en: ../data/processed/youtoxic_english_1000_augmented.csv\n",
      "\n",
      "   EstadÃ­sticas:\n",
      "   Total: 1909 ejemplos\n",
      "   Originales: 1000\n",
      "   Aumentados: 909\n",
      "   TÃ³xicos: 878\n",
      "   No tÃ³xicos: 1031\n"
     ]
    }
   ],
   "source": [
    "# Guardar dataset aumentado\n",
    "output_path = Path('../data/processed/youtoxic_english_1000_augmented.csv')\n",
    "\n",
    "# Preparar para guardar (usar nombres de columnas originales)\n",
    "df_to_save = df_augmented.copy()\n",
    "df_to_save['Text'] = df_to_save['text']\n",
    "df_to_save['IsToxic'] = df_to_save['label'].astype(int)\n",
    "\n",
    "# Guardar solo columnas necesarias\n",
    "df_to_save[['Text', 'IsToxic', '_augmented', '_augmentation_method']].to_csv(\n",
    "    output_path,\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"âœ… Dataset aumentado guardado en: {output_path}\")\n",
    "print(f\"\\n   EstadÃ­sticas:\")\n",
    "print(f\"   Total: {len(df_to_save)} ejemplos\")\n",
    "print(f\"   Originales: {len(df_to_save[~df_to_save['_augmented']])}\")\n",
    "print(f\"   Aumentados: {len(df_to_save[df_to_save['_augmented']])}\")\n",
    "print(f\"   TÃ³xicos: {df_to_save['IsToxic'].sum()}\")\n",
    "print(f\"   No tÃ³xicos: {len(df_to_save) - df_to_save['IsToxic'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Vectorizar y Entrenar Modelo con Dataset Aumentado (Opcional)\n",
    "\n",
    "> **Nota**: Esta secciÃ³n es opcional. Puedes entrenar el modelo con el dataset aumentado y comparar mÃ©tricas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Vectorizando dataset aumentado...\n",
      "âœ… VectorizaciÃ³n completada:\n",
      "   Train: (1527, 5000)\n",
      "   Test: (382, 5000)\n",
      "   Features: 5000\n"
     ]
    }
   ],
   "source": [
    "# Vectorizar dataset aumentado\n",
    "print(\"ðŸ”„ Vectorizando dataset aumentado...\")\n",
    "\n",
    "# Preparar datos\n",
    "X_aug = df_to_save['Text'].values\n",
    "y_aug = df_to_save['IsToxic'].values\n",
    "\n",
    "# Crear vectorizador y entrenar\n",
    "vectorizer = TextVectorizer(method='tfidf', max_features=5000)\n",
    "X_aug_vectorized = vectorizer.fit_transform(pd.Series(X_aug))\n",
    "\n",
    "# Split train/test\n",
    "X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(\n",
    "    X_aug_vectorized,\n",
    "    y_aug,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_aug\n",
    ")\n",
    "\n",
    "print(f\"âœ… VectorizaciÃ³n completada:\")\n",
    "print(f\"   Train: {X_train_aug.shape}\")\n",
    "print(f\"   Test: {X_test_aug.shape}\")\n",
    "print(f\"   Features: {X_train_aug.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusiones\n",
    "\n",
    "### Resultados del Data Augmentation:\n",
    "\n",
    "1. **TamaÃ±o del dataset**: Â¿Se duplicÃ³ correctamente?\n",
    "2. **Balance de clases**: Â¿Se mantiene el balance?\n",
    "3. **Calidad del texto aumentado**: Â¿Los sinÃ³nimos son apropiados?\n",
    "\n",
    "### Recomendaciones:\n",
    "\n",
    "- Si mejora mÃ©tricas: Usar dataset aumentado para entrenar modelo final\n",
    "- Si no mejora: Investigar otras tÃ©cnicas de augmentation\n",
    "- Considerar: Aumentar solo la clase minoritaria para balancear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aumentar Dataset Completo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
