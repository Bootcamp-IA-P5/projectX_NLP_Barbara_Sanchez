# Resultados del Feature Engineering / Vectorizaci√≥n
## Dataset: YouToxic English 1000 - Preprocesado

Este documento resume los resultados y configuraciones del proceso de vectorizaci√≥n aplicado al texto preprocesado para preparar los datos para el modelado de Machine Learning.

---

## 1. Objetivo

Convertir el texto preprocesado en representaciones num√©ricas (vectores) que los modelos de Machine Learning puedan procesar. Se comparan dos m√©todos de vectorizaci√≥n: **TF-IDF** y **Count Vectorizer (Bag of Words)**.

---

## 2. Divisi√≥n de Datos

### Estrategia
- **M√©todo**: `train_test_split` con estratificaci√≥n
- **Proporci√≥n**: 80% entrenamiento / 20% prueba
- **Random State**: 42 (para reproducibilidad)
- **Stratify**: S√≠ (mantiene la proporci√≥n de clases)

### Resultados Esperados
- **Conjunto de entrenamiento**: ~800 comentarios (80%)
- **Conjunto de prueba**: ~200 comentarios (20%)
- **Balance de clases**: Mantiene la proporci√≥n original (~46% t√≥xicos, ~54% no t√≥xicos)

### Importancia
- La estratificaci√≥n asegura que ambos conjuntos tengan la misma distribuci√≥n de clases
- Esto es crucial para evaluar correctamente el rendimiento del modelo

---

## 3. Vectorizaci√≥n con TF-IDF

### ¬øQu√© es TF-IDF?
**Term Frequency-Inverse Document Frequency** mide la importancia de cada palabra en cada documento:
- **TF (Term Frequency)**: Frecuencia de la palabra en el documento
- **IDF (Inverse Document Frequency)**: Penaliza palabras muy comunes en todo el corpus
- **Resultado**: Palabras √∫nicas e importantes tienen mayor peso

### Configuraci√≥n Aplicada

| Par√°metro | Valor | Descripci√≥n |
|-----------|-------|-------------|
| `max_features` | 5000 | Top 5000 palabras m√°s importantes |
| `ngram_range` | (1, 2) | Unigramas y bigramas (palabras individuales y pares) |
| `min_df` | 2 | Palabra debe aparecer en al menos 2 documentos |
| `max_df` | 0.95 | Ignora palabras en m√°s del 95% de documentos |
| `stop_words` | 'english' | Elimina stopwords (aunque ya fueron eliminadas en preprocesamiento) |

### Caracter√≠sticas
- **Tipo de matriz**: Sparse matrix (matriz dispersa)
- **Forma**: (n_comentarios, 5000 caracter√≠sticas)
- **Ventaja**: Da m√°s peso a palabras √∫nicas y relevantes
- **Uso**: Ideal cuando queremos destacar palabras distintivas

### Ejemplo de Caracter√≠sticas
Las caracter√≠sticas incluyen:
- **Unigramas**: Palabras individuales (ej: "toxic", "hate", "comment")
- **Bigramas**: Pares de palabras (ej: "toxic comment", "hate speech")

---

## 4. Vectorizaci√≥n con Count Vectorizer

### ¬øQu√© es Count Vectorizer?
**Bag of Words** cuenta simplemente cu√°ntas veces aparece cada palabra en cada documento:
- No penaliza palabras comunes
- Todas las palabras tienen el mismo peso inicial
- M√°s simple que TF-IDF

### Configuraci√≥n Aplicada

| Par√°metro | Valor | Descripci√≥n |
|-----------|-------|-------------|
| `max_features` | 5000 | Top 5000 palabras m√°s frecuentes |
| `ngram_range` | (1, 2) | Unigramas y bigramas |
| `min_df` | 2 | Palabra debe aparecer en al menos 2 documentos |
| `max_df` | 0.95 | Ignora palabras en m√°s del 95% de documentos |
| `stop_words` | 'english' | Elimina stopwords |

### Caracter√≠sticas
- **Tipo de matriz**: Sparse matrix (matriz dispersa)
- **Forma**: (n_comentarios, 5000 caracter√≠sticas)
- **Ventaja**: M√°s simple, captura frecuencia directa
- **Uso**: √ötil cuando la frecuencia simple es importante

---

## 5. Comparaci√≥n: TF-IDF vs Count Vectorizer

### Diferencias Principales

| Aspecto | TF-IDF | Count Vectorizer |
|---------|--------|------------------|
| **Peso de palabras** | Penaliza palabras comunes | Todas tienen peso igual |
| **Valores** | Normalizados (0-1) | Frecuencias enteras |
| **Complejidad** | Mayor | Menor |
| **Mejor para** | Palabras distintivas | Frecuencia directa |

### Densidad de Matrices
Ambas matrices son **dispersas** (sparse):
- Contienen muchos ceros (palabras que no aparecen en cada documento)
- Eficientes en memoria
- Scikit-learn las maneja autom√°ticamente

### Distribuci√≥n de Valores

**TF-IDF:**
- Valores normalizados entre 0 y 1
- Distribuci√≥n m√°s uniforme
- Valores m√°s peque√±os en promedio

**Count Vectorizer:**
- Valores enteros (frecuencias)
- Puede tener valores m√°s altos
- Distribuci√≥n m√°s sesgada hacia valores bajos

---

## 6. Archivos Generados

### Matrices Vectorizadas
- `X_train_tfidf.pkl` - Matriz TF-IDF de entrenamiento
- `X_test_tfidf.pkl` - Matriz TF-IDF de prueba
- `X_train_count.pkl` - Matriz Count Vectorizer de entrenamiento
- `X_test_count.pkl` - Matriz Count Vectorizer de prueba

### Vectorizadores Entrenados
- `tfidf_vectorizer.pkl` - Vectorizador TF-IDF entrenado (para usar en producci√≥n)
- `count_vectorizer.pkl` - Vectorizador Count Vectorizer entrenado

### Variables Objetivo
- `y_train.pkl` - Etiquetas de entrenamiento (IsToxic: 0 o 1)
- `y_test.pkl` - Etiquetas de prueba (IsToxic: 0 o 1)

### Ubicaci√≥n
- **Matrices y variables objetivo**: `data/processed/`
- **Vectorizadores**: `models/`

---

## 7. Decisiones de Dise√±o

### ¬øPor qu√© 5000 caracter√≠sticas?
- Balance entre informaci√≥n y eficiencia computacional
- Captura las palabras m√°s importantes sin sobrecargar el modelo
- Puede ajustarse seg√∫n resultados del modelado

### ¬øPor qu√© unigramas y bigramas?
- **Unigramas**: Capturan palabras individuales importantes
- **Bigramas**: Capturan frases y contextos (ej: "hate speech", "toxic comment")
- Combinaci√≥n mejora la capacidad de capturar significado

### ¬øPor qu√© min_df=2 y max_df=0.95?
- **min_df=2**: Elimina palabras muy raras (posibles errores de tipeo)
- **max_df=0.95**: Elimina palabras demasiado comunes (no informativas)
- Ayuda a reducir ruido y mejorar la calidad de las caracter√≠sticas

---

## 8. Pr√≥ximos Pasos

### Modelado
1. **Probar ambos m√©todos** con diferentes modelos de ML:
   - Naive Bayes
   - Logistic Regression
   - SVM (Support Vector Machine)
   - Random Forest

2. **Comparar rendimiento**:
   - ¬øTF-IDF o Count Vectorizer funciona mejor?
   - ¬øQu√© modelo se adapta mejor a cada m√©todo?

3. **Seleccionar el mejor**:
   - Combinaci√≥n de m√©todo de vectorizaci√≥n + modelo
   - Basado en m√©tricas: Accuracy, Precision, Recall, F1-score

### Optimizaci√≥n
- Ajustar hiperpar√°metros del mejor modelo
- Probar diferentes configuraciones de vectorizaci√≥n si es necesario
- Validar que no hay overfitting (diferencia < 5% entre train y test)

---

## 9. Consideraciones Importantes

### ‚úÖ Ventajas de la Estrategia
1. **Dos m√©todos comparables**: Misma configuraci√≥n permite comparaci√≥n justa
2. **Matrices guardadas**: No necesitamos re-vectorizar en cada experimento
3. **Vectorizadores guardados**: Listos para usar en producci√≥n
4. **Divisi√≥n estratificada**: Mantiene balance de clases

### ‚ö†Ô∏è Limitaciones
1. **Bag of Words**: No captura orden de palabras ni contexto largo
2. **5000 caracter√≠sticas**: Puede no capturar todas las palabras importantes
3. **Unigramas y bigramas**: No captura relaciones m√°s complejas

### üí° Mejoras Futuras (Nivel Avanzado)
- **Word Embeddings**: Word2Vec, GloVe, FastText
- **Transformers**: BERT, DistilBERT (requiere m√°s recursos)
- **N-gramas m√°s largos**: Trigramas, etc.

---

## 10. Resumen Ejecutivo

### Lo que hemos logrado:
‚úÖ Texto preprocesado convertido en representaciones num√©ricas  
‚úÖ Dos m√©todos de vectorizaci√≥n implementados y comparados  
‚úÖ Datos divididos en train/test de forma estratificada  
‚úÖ Matrices y vectorizadores guardados para reutilizaci√≥n  
‚úÖ 5000 caracter√≠sticas por comentario (unigramas + bigramas)  

### Estado actual:
- **Listo para modelado**: Las matrices est√°n preparadas
- **Comparaci√≥n pendiente**: Necesitamos probar con modelos para ver cu√°l funciona mejor
- **Siguiente paso**: Entrenar modelos con ambas representaciones

---


